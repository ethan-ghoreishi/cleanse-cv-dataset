{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk.tokenize as nt\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from textblob import blob, Blobber, TextBlob, Sentence, Word, WordList, tokenizers, sentiments, taggers, parsers\n",
    "#from textblob_aptagger import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(dbname=\"skillsdb\",host=\"\"\n",
    "                ,port=\"\",user=\"\", password=\"\")\n",
    "curs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   5049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of CVs\n",
    "\n",
    "pd.read_sql_query('''select count(distinct user_id) from cv''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query('''select distinct cv.user_id, cv_section_attribute.name,\n",
    "cv.value_char, cv.value_timestamp from cv_section_attribute \n",
    "left join cv on cv_section_attribute.id=cv.cv_section_attribute_id''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['value_char'] = data['value_char'].map(lambda x: x.strip() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = data[data['name'].isin(['locale','name','summary','headline',\n",
    "                                     'degree','school','admit_year','grad_year',\n",
    "                                     'company',  'title',  'work_location',  \n",
    "                                     'start_date','end_date', 'description',\n",
    "                                     'award',\n",
    "                                     'publication', \n",
    "                                     'additional_info', \n",
    "                                     'skill'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tokenization parameters\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "tokenizer_prefs = {\n",
    "    'tokenizer' : nltk.tokenize.PunktSentenceTokenizer(),\n",
    "#     'token_format' : 'stem',\n",
    "    'spell_correct' : False,\n",
    "    'np_extract': None,\n",
    "    'pos_tagger': None,\n",
    "    'analyzer': None,\n",
    "    'classifier': None, \n",
    "    'clean_html': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(**kwargs):\n",
    "    '''\n",
    "    Cleans text data by:\n",
    "    1.  force lowercase\n",
    "    2.  _ non-ascii chars\n",
    "    3.  standardize whitespace\n",
    "    4.  remove digits\n",
    "    5.  remove control characters\n",
    "    6.  remove URL patterns\n",
    "    '''\n",
    "    df = pd.DataFrame(data_valid)\n",
    "    \n",
    "    try:\n",
    "        df['value_char'] = data_valid['value_char'].dropna().map(lambda x: \"\".join(i for i in x.strip().lower() if ord(i)<128))\n",
    "    except UnicodeDecodeError:\n",
    "        print(UnicodeDecodeError)\n",
    "        df['value_char'] = data_valid['value_char'].dropna().map(lambda x: x.strip().lower())\n",
    "\n",
    "\n",
    "    url_pattern = \"((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\"\n",
    "\n",
    "    re_URL = re.compile(url_pattern)\n",
    "#     re_TAG = re.compile(\"(<[phl]>)\", re.IGNORECASE)\n",
    "    re_WS = re.compile(\"/[^\\S\\n]/\")\n",
    "#     re_DIGIT = re.compile(\"\\d\")\n",
    "    re_CTRL = re.compile(\"[\\x00-\\x11\\x03-\\x1F]+\")\n",
    "    re_HI = re.compile(\"[\\x80-\\xFF]+\")\n",
    "    re_NWC = re.compile(\"[!;<>?{}\\/~`#=@#$%^&*()_+]\")\n",
    "    \n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_HI.sub(' ', x) if type(x) == str else None)\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_CTRL.sub(' ', x) if type(x) == str else None)\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_URL.sub(' ', x) if type(x) == str else None)\n",
    "#     data[prefix] = data[prefix].map(lambda x: re_DIGIT.sub(' ', x))\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_WS.sub(' ', x) if type(x) == str else None)        \n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_NWC.sub(' ', x) if type(x) == str else None)\n",
    "    \n",
    "\n",
    "    # create a blon using TextBlob\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    pos_tagger = kwargs['pos_tagger']\n",
    "    analyzer = kwargs['analyzer']\n",
    "    classifier = kwargs['classifier']\n",
    "    np_extract = kwargs['np_extract']\n",
    "    \n",
    "    df['value_char'] = df['value_char'].map(lambda l: TextBlob(l,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                           np_extractor=np_extract,\n",
    "                                           pos_tagger=pos_tagger,\n",
    "                                           analyzer=analyzer) if l is not None else None)\n",
    "\n",
    "    df_sentences = pd.DataFrame(df)\n",
    "    # tokenize the document into sentences from blob object\n",
    "    df_sentences['value_char'] = df['value_char'].map(lambda s: s.sentences if s is not None else None)\n",
    "    \n",
    "    df_words = pd.DataFrame(df_sentences)\n",
    "    # tokenize each sentence into words\n",
    "    df_words['value_char'] = df_sentences['value_char'].dropna().map(lambda l: (w.strip().words for w in l if w is not None and len(w)>1))\n",
    "    \n",
    "    \n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_normalized = normalize_data(data_word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(target):\n",
    "    vocab = set()\n",
    "#     if not isinstance(df, type(None)):\n",
    "    for token in df.value_char:\n",
    "        if not isinstance(token, type(None)) and type(token) !=float:\n",
    "            for sentence in token:\n",
    "                if not isinstance(sentence, type(None)) and type(sentence) !=float:\n",
    "                    for word in sentence:\n",
    "#                         if not isinstance(word, type(None)) and word not in stopWords and word != '\\'s' and word != '\\'d':\n",
    "                         if not isinstance(word, type(None)) and word != '\\'s' and word != '\\'d':\n",
    "                            vocab.add(word)\n",
    "            \n",
    "    if target:\n",
    "        w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "        w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    else:\n",
    "        w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "    return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = build_vocab(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'omdurmanuiversity2011': 2,\n",
       " 'interviewereffective': 1018,\n",
       " ...}"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(data_normalized, w2i):\n",
    "    encoded_sentence = []\n",
    "    if not isinstance(data_normalized, type(None)):\n",
    "        for token in data_normalized:\n",
    "            if not isinstance(token, type(None)):\n",
    "                for sentence in token:\n",
    "                    if not isinstance(sentence, type(None)):\n",
    "                        for w in sentence:\n",
    "                            try:\n",
    "                                encoded_sentence.append(w2i[w])\n",
    "                            except Exception:\n",
    "                                pass\n",
    "    return encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34587,\n",
       " ...]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(data_normalized, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_normalized, vocab=None, w2i=None, target=True):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(data_normalized, target)\n",
    "\n",
    "    s = []\n",
    "    data = []\n",
    "    if not isinstance(data_normalized, type(None)):\n",
    "        for token in data_normalized:\n",
    "            if not isinstance(token, type(None)):\n",
    "                for sentence in token:\n",
    "                    if not isinstance(sentence, type(None)):\n",
    "                        for w in sentence:\n",
    "                            s.append(w)\n",
    "        if target:\n",
    "            s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(data_normalized,w2i=w2i,target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "if not isinstance(data_normalized, type(None)):\n",
    "    for token in data_normalized:\n",
    "        if not isinstance(token, type(None)):\n",
    "            for sentence in token:\n",
    "                if not isinstance(sentence, type(None)):\n",
    "                    for s in sentence:\n",
    "                        data_all.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: map(lambda w: nltk.stem.PorterStemmer.NLTK_EXTENSIONS(w) if w in wl and not isinstance(w, type(None)) else wl.remove(w), wl), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in data_normalized:\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                for l in k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeNoneTypes(lst):\n",
    "    return [i for i in lst if type(i) is not type(None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_word_tokenized, **kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    \n",
    "    data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: removeNoneTypes(wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda w: w.singularize(), l))\n",
    "    \n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    " \n",
    "     # remove tokens with length 1\n",
    "         wl_coll = list()\n",
    "         for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "            for j in i:\n",
    "                if not isinstance(j, type(None)):\n",
    "                    for k in j:\n",
    "                        if not isinstance(k, type(None)):\n",
    "                            if re.match(ree, i):\n",
    "                                ree.sub('', i)\n",
    "                            if len(i.strip().strip('.').strip(',')) > 1:\n",
    "                                wl.append((i))\n",
    "                    wl_coll.append(WordList(wl))\n",
    "            data_normalized[indx] = wl_col\n",
    "            del tmp\n",
    "\n",
    "    # stemming\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: nltk.stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "\n",
    "    data_word_tokenized= tokenize_words('value_char', **tokenizer_prefs)\n",
    "    \n",
    "    return data_word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "tokenizer  =   RegexpTokenizer(pattern=r'\\w+')\n",
    "stemmer    =   nltk.stem.PorterStemmer.NLTK_EXTENSIONS\n",
    "lemmatize  =   nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(data_word_tokenized,**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_data(**kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    aa = cleanestes.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    "    # Stemming or lemmatization of tokens    \n",
    "    if normalizer == 'stem':\n",
    "        aa = aa.map(lambda l: map(lambda wl: map(lambda w: stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(Word, wl), l))\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(**tokenizer_prefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
