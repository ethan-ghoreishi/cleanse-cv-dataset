{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk.tokenize as nt\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from textblob import blob, Blobber, TextBlob, Sentence, Word, WordList, tokenizers, sentiments, taggers, parsers\n",
    "#from textblob_aptagger import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(dbname=\"skillsdb\",host=\"dw-instance.cbrlhmbtfrqg.eu-west-2.redshift.amazonaws.com\"\n",
    "                ,port=\"5439\",user=\"masteruser\", password=\"Ehgh1363\")\n",
    "curs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   5049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of CVs\n",
    "\n",
    "pd.read_sql_query('''select count(distinct user_id) from cv''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_sql_query('''select distinct cv.user_id, cv_section_attribute.name,\n",
    "cv.value_char, cv.value_timestamp from cv_section_attribute \n",
    "left join cv on cv_section_attribute.id=cv.cv_section_attribute_id''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data['value_char'] = data['value_char'].map(lambda x: x.strip() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_valid = data[data['name'].isin(['locale','name','summary','headline',\n",
    "                                     'degree','school','admit_year','grad_year',\n",
    "                                     'company',  'title',  'work_location',  \n",
    "                                     'start_date','end_date', 'description',\n",
    "                                     'award',\n",
    "                                     'publication', \n",
    "                                     'additional_info', \n",
    "                                     'skill'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_headlines = data[data['name'] == 'headline'][['user_id','value_char']]\n",
    "# cv_degrees = data[data['name'] == 'degree'][['user_id','value_char']]\n",
    "# cv_schools = data[data['name'] == 'school'][['user_id','value_char']]\n",
    "# cv_locales = data[data['name'] == 'locale'][['user_id','value_char']]\n",
    "# cv_summaries = data[data['name'] == 'summary'][['user_id','value_char']]\n",
    "# cv_companies = data[data['name'] == 'company'][['user_id','value_char']]\n",
    "# cv_titles = data[data['name'] == 'title'][['user_id','value_char']]\n",
    "# cv_work_locations = data[data['name'] == 'work_location'][['user_id','value_char']]\n",
    "# cv_descriptions = data[data['name'] == 'work_description'][['user_id','value_char']]\n",
    "# cv_awards = data[data['name'] == 'award'][['user_id','value_char']]\n",
    "# cv_skills = data[data['name'] == 'skill'][['user_id','value_char']]\n",
    "# cv_add_info = data[data['name'] == 'additional_info'][['user_id','value_char']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_headlines.merge(\n",
    "#     cv_summaries, how='outer', on='user_id').merge(\n",
    "#     cv_locales, how='outer', on='user_id').merge(cv_awards, how='outer', on='user_id').merge(cv_add_info, how='outer', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tokenization parameters\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "tokenizer_prefs = {\n",
    "    'tokenizer' : nltk.tokenize.PunktSentenceTokenizer(),\n",
    "#     'token_format' : 'stem',\n",
    "    'spell_correct' : False,\n",
    "    'np_extract': None,\n",
    "    'pos_tagger': None,\n",
    "    'analyzer': None,\n",
    "    'classifier': None, \n",
    "    'clean_html': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(**kwargs):\n",
    "    '''\n",
    "    Cleans text data by:\n",
    "    1.  force lowercase\n",
    "    2.  remove non-ascii chars\n",
    "    3.  standardize whitespace\n",
    "    4.  remove digits\n",
    "    5.  remove control characters\n",
    "    6.  remove URL patterns\n",
    "    '''\n",
    "    df = pd.DataFrame(data_valid)\n",
    "    \n",
    "    try:\n",
    "        df['value_char'] = data_valid['value_char'].dropna().map(lambda x: \"\".join(i for i in x.strip().lower() if ord(i)<128))\n",
    "    except UnicodeDecodeError:\n",
    "        print(UnicodeDecodeError)\n",
    "        df['value_char'] = data_valid['value_char'].dropna().map(lambda x: x.strip().lower())\n",
    "\n",
    "\n",
    "    url_pattern = \"((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\"\n",
    "\n",
    "    re_URL = re.compile(url_pattern)\n",
    "#     re_TAG = re.compile(\"(<[phl]>)\", re.IGNORECASE)\n",
    "    re_WS = re.compile(\"/[^\\S\\n]/\")\n",
    "#     re_DIGIT = re.compile(\"\\d\")\n",
    "    re_CTRL = re.compile(\"[\\x00-\\x11\\x03-\\x1F]+\")\n",
    "    re_HI = re.compile(\"[\\x80-\\xFF]+\")\n",
    "    re_NWC = re.compile(\"[!;<>?{}\\/~`#=@#$%^&*()_+]\")\n",
    "    \n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_HI.sub(' ', x) if type(x) == str else None)\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_CTRL.sub(' ', x) if type(x) == str else None)\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_URL.sub(' ', x) if type(x) == str else None)\n",
    "#     data[prefix] = data[prefix].map(lambda x: re_DIGIT.sub(' ', x))\n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_WS.sub(' ', x) if type(x) == str else None)        \n",
    "    df['value_char'] = df['value_char'].map(lambda x: re_NWC.sub(' ', x) if type(x) == str else None)\n",
    "    \n",
    "\n",
    "    # create a blon using TextBlob\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    pos_tagger = kwargs['pos_tagger']\n",
    "    analyzer = kwargs['analyzer']\n",
    "    classifier = kwargs['classifier']\n",
    "    np_extract = kwargs['np_extract']\n",
    "    \n",
    "    df['value_char'] = df['value_char'].map(lambda l: TextBlob(l,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                           np_extractor=np_extract,\n",
    "                                           pos_tagger=pos_tagger,\n",
    "                                           analyzer=analyzer) if l is not None else None)\n",
    "\n",
    "    df_sentences = pd.DataFrame(df)\n",
    "    # tokenize the document into sentences from blob object\n",
    "    df_sentences['value_char'] = df['value_char'].map(lambda s: s.sentences if s is not None else None)\n",
    "    \n",
    "    df_words = pd.DataFrame(df_sentences)\n",
    "    # tokenize each sentence into words\n",
    "#     df_words = df_sentences['value_char'].map(lambda l: map(lambda w: w.strip().words if w is not None and len(w)>1 else None, l))\n",
    "    df_words['value_char'] = df_sentences['value_char'].dropna().map(lambda l: (w.strip().words for w in l if w is not None and len(w)>1))\n",
    "    \n",
    "    \n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = clean_data(**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer = tokenizer_prefs['tokenizer']\n",
    "# pos_tagger = tokenizer_prefs['pos_tagger']\n",
    "# analyzer = tokenizer_prefs['analyzer']\n",
    "# classifier = tokenizer_prefs['classifier']\n",
    "# np_extract = tokenizer_prefs['np_extract']\n",
    "\n",
    "# b = df['value_char'].map(lambda l: TextBlob(l,\n",
    "#                                       tokenizer=tokenizer,\n",
    "#                                        np_extractor=np_extract,\n",
    "#                                        pos_tagger=pos_tagger,\n",
    "#                                        analyzer=analyzer,\n",
    "#                                        classifier=classifier) if l is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_valid = data_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Tokenization\n",
    "\n",
    "# # initialize the tokenizer\n",
    "\n",
    "# tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "# # tokenize data\n",
    "\n",
    "# data_clean['value_char'] = data_clean['value_char'].map(lambda x: tokenizer.tokenize(x.strip()))\n",
    "# data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word_tokenizer = nltk.tokenize.word_tokenize()\n",
    "# data_clean['value_char'] = data_clean['value_char'].map(lambda x: nltk.tokenize.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Stemming\n",
    "\n",
    "# # initialize the stemmer\n",
    "\n",
    "# stemmer = nltk.stem.PorterStemmer()\n",
    "# # data_clean['value_char'].map(lambda x: (i for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatize = nltk.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_blobs = pd.DataFrame()\n",
    "# data_blobs['value_char'] = create_blob('value_char', **tokenizer_prefs)\n",
    "# data_blobs.value_char[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tokenize_sentences(prefix):\n",
    "# #     tokenizer = kwargs['tokenizer']\n",
    "# #     normalizer = kwargs['token_format']\n",
    "\n",
    "#     # tokenize the document into sentences from blob object\n",
    "#     sentences = data_blobs[prefix].map(lambda s: s.sentences)\n",
    "\n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_sentence_tokenized = pd.DataFrame()\n",
    "# data_sentence_tokenized['value_char'] = tokenize_sentences('value_char')\n",
    "# data_sentence_tokenized.value_char[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def tokenize_words(prefix):\n",
    "# #     tokenizer = kwargs['tokenizer']\n",
    "# #     normalizer = kwargs['token_format']\n",
    "\n",
    "#     # tokenize each sentence into words\n",
    "#     # trim token whitespaces\n",
    "#     # eliminate tokens of character length 1\n",
    "#     #words = self.data[prefix].map(lambda w: w.strip().tokens if len(w)>1 else None)\n",
    "\n",
    "#     words = data_sentence_tokenized[prefix].map(lambda l: map(lambda w: w.strip().words if len(w)>1 else None, l))\n",
    "\n",
    "#     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_word_tokenized = pd.DataFrame()\n",
    "# data_word_tokenized = tokenize_words('value_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def normalize_data(data_word_tokenized):\n",
    "# #     tokenizer = kwargs['tokenizer']\n",
    "# #     normalizer = kwargs['token_format']\n",
    "# #     spelling = kwargs['spell_correct']\n",
    "    \n",
    "#     data_normalized = data_word_tokenized.map(lambda l: map(lambda w: w.singularize() if not isinstance(w, type(None)) and len(w)>1 else None, l))\n",
    "# #     data_normalized = data_normalized.dropna()\n",
    "    \n",
    "#     # filter out 'bad' words, normalize good ones\n",
    "#     # w if w not in self.stopWords else wl.remove(w)\n",
    "# #     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords and not isinstance(w, type(None)) else w, wl), l))\n",
    "# #     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' and not isinstance(w, type(None) else w, wl), l))\n",
    "# #     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' and not isinstance(w, type(None) else w, wl), l))\n",
    "# #     data_normalized = data_normalized.dropna()\n",
    "\n",
    "#      # remove tokens with length 1\n",
    "# #     ree = re.compile(r'(\\'\\w)')\n",
    "# #     rlen = len(data_normalized)\n",
    "# #     tmp = data_normalized.copy()\n",
    "# # #     tmp.reset_index(drop=True)\n",
    "# #     for indx in tmp.index:\n",
    "# #          wl_coll = list()\n",
    "# #          for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "# #             for j in i:\n",
    "# #                 if not isinstance(j, type(None)):\n",
    "# #                     for k in j:\n",
    "# #                         if not isinstance(k, type(None)):\n",
    "# #                             if re.match(ree, i):\n",
    "# #                                 ree.sub('', i)\n",
    "# #                             if len(i.strip().strip('.').strip(',')) > 1:\n",
    "# #                                 wl.append((i))\n",
    "# #                     wl_coll.append(WordList(wl))\n",
    "# #             data_normalized[indx] = wl_col\n",
    "# #             del tmp\n",
    "\n",
    "#     # stemming\n",
    "# #     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: nltk.stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "\n",
    "# #     data_word_tokenized= tokenize_words('value_char', **tokenizer_prefs)\n",
    "    \n",
    "#     return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_normalized = normalize_data(data_word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if not isinstance(data_normalized, type(None)):\n",
    "#     for i in data_normalized:\n",
    "#         if not isinstance(i, type(None)):\n",
    "#             for j in i:\n",
    "#                 if not isinstance(j, type(None)):\n",
    "#                     for k in j:\n",
    "#                         if not isinstance(k, type(None)) and k not in stopWords and k != '\\'s' and k != '\\'d':\n",
    "#                             print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(target):\n",
    "    vocab = set()\n",
    "#     if not isinstance(df, type(None)):\n",
    "    for token in df.value_char:\n",
    "        if not isinstance(token, type(None)) and type(token) !=float:\n",
    "            for sentence in token:\n",
    "                if not isinstance(sentence, type(None)) and type(sentence) !=float:\n",
    "                    for word in sentence:\n",
    "#                         if not isinstance(word, type(None)) and word not in stopWords and word != '\\'s' and word != '\\'d':\n",
    "                         if not isinstance(word, type(None)) and word != '\\'s' and word != '\\'d':\n",
    "                            vocab.add(word)\n",
    "            \n",
    "    if target:\n",
    "        w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "        w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    else:\n",
    "        w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "    return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2i = build_vocab(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'omdurmanuiversity2011': 2,\n",       
       " 'interviewereffective': 1018,\n",
       " ...}"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(data_normalized, w2i):\n",
    "    encoded_sentence = []\n",
    "    if not isinstance(data_normalized, type(None)):\n",
    "        for token in data_normalized:\n",
    "            if not isinstance(token, type(None)):\n",
    "                for sentence in token:\n",
    "                    if not isinstance(sentence, type(None)):\n",
    "                        for w in sentence:\n",
    "                            try:\n",
    "                                encoded_sentence.append(w2i[w])\n",
    "                            except Exception:\n",
    "                                pass\n",
    "    return encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34587,\n",
       " ...]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(data_normalized, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_normalized, vocab=None, w2i=None, target=True):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(data_normalized, target)\n",
    "\n",
    "    s = []\n",
    "    data = []\n",
    "    if not isinstance(data_normalized, type(None)):\n",
    "        for token in data_normalized:\n",
    "            if not isinstance(token, type(None)):\n",
    "                for sentence in token:\n",
    "                    if not isinstance(sentence, type(None)):\n",
    "                        for w in sentence:\n",
    "                            s.append(w)\n",
    "        if target:\n",
    "            s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[41997,\n",
       "   ...]],\n",
       " {'hypothesi': 2,\n",
       "  ...},\n",
       " {0: '<s>',\n",
       "  1: '</s>',\n",
       "  2: 'hypothesi',\n",
       "  3: 'etching',\n",
       "  4: 'orthopedist',\n",
       "  5: 'premiere-based',\n",
       "  6: 'perday',\n",
       "  7: 'andposition',\n",
       "  8: 'outlay',\n",
       "  9: 'dorado',\n",
       "  10: 'lorry',\n",
       "  11: 'semisolid',\n",
       "  12: '2013tinatus',\n",
       "  13: 'sister',\n",
       "  14: 'became',\n",
       "  15: 'geometryal-qud',\n",
       "  16: 'vmsimplementing',\n",
       "  17: 'sh',\n",
       "  18: 'rutgers-the',\n",
       "  19: 'dorotocephala',\n",
       "  20: 'windowsapplication',\n",
       "  21: '2015led',\n",
       "  22: 'othersfocused',\n",
       "  23: '429',\n",
       "  24: 'benefitsresource',\n",
       "  25: 'data-cabling',\n",
       "  26: 'developmentfinancial',\n",
       "  27: 'worker-therapist',\n",
       "  28: 'reportsquality',\n",
       "  29: 'solutionsdesigned',\n",
       "  30: 'steward',\n",
       "  31: 'teachingdecember',\n",
       "  32: 'skava',\n",
       "  33: 'aif',\n",
       "  34: 'analysisconsultantsagile',\n",
       "  35: '2016hillingdon',\n",
       "  36: 'r2application',\n",
       "  37: 'captioning',\n",
       "  38: 'deliving',\n",
       "  39: 't2',\n",
       "  40: 'abi',\n",
       "  41: 'compounddissolution',\n",
       "  42: 'syspro',\n",
       "  43: 'emphasize',\n",
       "  44: \"'student\",\n",
       "  45: 'arts-political',\n",
       "  46: 'educationgrade',\n",
       "  47: 'sapperson',\n",
       "  48: 'severely',\n",
       "  49: 'stagecommunicating',\n",
       "  50: 'best-of',\n",
       "  51: 'instance',\n",
       "  52: 'servicesprofessional',\n",
       "  53: 'accountingdocument',\n",
       "  54: 'hello',\n",
       "  55: '250inmate',\n",
       "  56: 'continue',\n",
       "  57: 'clr',\n",
       "  58: 'perk',\n",
       "  59: 'transferrable',\n",
       "  60: 'bike',\n",
       "  61: '2011financial',\n",
       "  62: 'neurologic',\n",
       "  63: 'customizing',\n",
       "  64: 's26responsibility',\n",
       "  65: 'analysisof',\n",
       "  66: 'submitted',\n",
       "  67: 'rfu',\n",
       "  68: 'kadry',\n",
       "  69: 'qtpsoftware',\n",
       "  70: 'caused',\n",
       "  71: 'freelancetrainer',\n",
       "  72: 'aktum',\n",
       "  73: 'statewide',\n",
       "  74: 'supportinginformation',\n",
       "  75: 'colorectal',\n",
       "  76: 'ambrosium',\n",
       "  77: 'pro-on',\n",
       "  78: 'london-based',\n",
       "  79: 'mastcraft',\n",
       "  80: 'undermanned',\n",
       "  81: 'wing',\n",
       "  82: 'git',\n",
       "  83: 'capitalanalysi',\n",
       "  84: 'roleproven',\n",
       "  85: 'stroke',\n",
       "  86: 'bramall',\n",
       "  87: 'bi-direction',\n",
       "  88: 'sputum',\n",
       "  89: 'hauler',\n",
       "  90: 'strict',\n",
       "  91: '2014undergraduate',\n",
       "  92: 'microreactor',\n",
       "  93: 'elaborating',\n",
       "  94: 'e-signal',\n",
       "  95: 'international',\n",
       "  96: 'achievablestandard',\n",
       "  97: 'responsible',\n",
       "  98: 'richfield',\n",
       "  99: 'infectiou',\n",
       "  100: 'ms-accesstesting',\n",
       "  101: '627',\n",
       "  102: '89-94',\n",
       "  103: 'oc4j',\n",
       "  104: 'mccarthy',\n",
       "  105: 'mcandrew',\n",
       "  106: 'bailey',\n",
       "  107: 'pythonmicrosoft',\n",
       "  108: 'plug-in',\n",
       "  109: 'notable',\n",
       "  110: 'ccp3',\n",
       "  111: '15614',\n",
       "  112: 'bluejuly',\n",
       "  113: 'squarespace',\n",
       "  114: 'labconnect',\n",
       "  115: 'astute',\n",
       "  116: 'shipsformalized',\n",
       "  117: 'multi-warehouseenvironment',\n",
       "  118: 'vision',\n",
       "  119: 'guidechairing',\n",
       "  120: 'nexusand',\n",
       "  121: 'jnt',\n",
       "  122: 'yellowdot',\n",
       "  123: 'confrontation',\n",
       "  124: 'flushed',\n",
       "  125: 'pressured',\n",
       "  126: 'disassembler',\n",
       "  127: 'defending',\n",
       "  128: 'agarwal',\n",
       "  129: 'superficial',\n",
       "  130: 'straus',\n",
       "  131: 'blocker',\n",
       "  132: 'hong',\n",
       "  133: 'jagiellonian',\n",
       "  134: 'accountscommunicated',\n",
       "  135: 'wicker',\n",
       "  136: 'sanef',\n",
       "  137: 'mef',\n",
       "  138: 'nut',\n",
       "  139: 'shukla',\n",
       "  140: 'short-sell',\n",
       "  141: 'seal',\n",
       "  142: 'mcpconfiguring',\n",
       "  143: 'bucharestduty',\n",
       "  144: 'akademie',\n",
       "  145: 'kmhisi',\n",
       "  146: 'dataand',\n",
       "  147: 'benefitmanagement',\n",
       "  148: 'lumension',\n",
       "  149: 'plasmonic',\n",
       "  150: 'mention',\n",
       "  151: 'science-chemical',\n",
       "  152: '10-cm',\n",
       "  153: 'inflicted',\n",
       "  154: 'toilet',\n",
       "  155: 'hawkin',\n",
       "  156: 'a3500austin',\n",
       "  157: 'functionsn',\n",
       "  158: 'immediately',\n",
       "  159: 'learnerhighly',\n",
       "  160: 'tmt',\n",
       "  161: 'r2r',\n",
       "  162: 'enviable',\n",
       "  163: 'bookersduty',\n",
       "  164: 'townhouse',\n",
       "  165: 'booking',\n",
       "  166: 'antineoplastic',\n",
       "  167: 'byjanuary',\n",
       "  168: 'monetory',\n",
       "  169: 'itn',\n",
       "  170: 'silva',\n",
       "  171: 'lambert',\n",
       "  172: 'offlorida',\n",
       "  173: 'dated',\n",
       "  174: 'impedance',\n",
       "  175: 'pvc',\n",
       "  176: 'mclaren',\n",
       "  177: 'engl',\n",
       "  178: 'learn',\n",
       "  179: 'jarrold',\n",
       "  180: 'reductase',\n",
       "  181: 'investing',\n",
       "  182: 'philadelphium',\n",
       "  183: 'telcordium',\n",
       "  184: 'divinitati',\n",
       "  185: 'viewing',\n",
       "  186: '64th',\n",
       "  187: 'short-term',\n",
       "  188: 'cadd',\n",
       "  189: 'redirecting',\n",
       "  190: 'electronicmedical',\n",
       "  191: 'developmenttechnology',\n",
       "  192: 'tsafleb',\n",
       "  193: 'skillsexpert',\n",
       "  194: 'eagerly',\n",
       "  195: '23m',\n",
       "  196: 'bishop',\n",
       "  197: 'h2o',\n",
       "  198: 'externalongoing',\n",
       "  199: 'clean-desk',\n",
       "  200: 'average',\n",
       "  201: '55th',\n",
       "  202: 'welding',\n",
       "  203: 'strove',\n",
       "  204: 'wrangling',\n",
       "  205: '7798731067',\n",
       "  206: 'vivendi',\n",
       "  207: 'pentagon',\n",
       "  208: 'kingdomnovember',\n",
       "  209: 'diverting',\n",
       "  210: 'uphold',\n",
       "  211: 'wal-mart',\n",
       "  212: 'certificateit',\n",
       "  213: 'harrisburg',\n",
       "  214: 'vliegende',\n",
       "  215: 'uh',\n",
       "  216: 'experienceunix',\n",
       "  217: 'mcauley',\n",
       "  218: 'underwriter',\n",
       "  219: 'phosphospermine',\n",
       "  220: 'trud',\n",
       "  221: 'for50',\n",
       "  222: 'reagent',\n",
       "  223: 'fast',\n",
       "  224: 'cognate',\n",
       "  225: 'officedelivering',\n",
       "  226: 'online',\n",
       "  227: 'ministerial',\n",
       "  228: 'point-of',\n",
       "  229: 'iiipace',\n",
       "  230: 'remington',\n",
       "  231: 'propose',\n",
       "  232: 'time-by',\n",
       "  233: 'fbusines',\n",
       "  234: 'multilingual',\n",
       "  235: 'ending',\n",
       "  236: 'makongorosi',\n",
       "  237: 'phonetically',\n",
       "  238: 'thermo-fluid',\n",
       "  239: 're-pricing',\n",
       "  240: 'eventreporting',\n",
       "  241: 'eproject',\n",
       "  242: 'siebe',\n",
       "  243: 'clearing',\n",
       "  244: 'gida',\n",
       "  245: 'freelancer',\n",
       "  246: 'rmysql',\n",
       "  247: 'ensheathing',\n",
       "  248: 'judging',\n",
       "  249: 'university-metropolitan',\n",
       "  250: 'newbusines',\n",
       "  251: 'kuznetsov',\n",
       "  252: 'centeravaloq',\n",
       "  253: 'accountantconvenience',\n",
       "  254: 'amtrust',\n",
       "  255: 'sur',\n",
       "  256: 'kitnlp',\n",
       "  257: 'exceeded',\n",
       "  258: 'sessional',\n",
       "  259: 'mid1',\n",
       "  260: 'oncology',\n",
       "  261: 'analysislanguage',\n",
       "  262: 'gdansk',\n",
       "  263: 'gem',\n",
       "  264: 'cpuc',\n",
       "  265: 'englishadditional',\n",
       "  266: 'looming',\n",
       "  267: 'grigg',\n",
       "  268: 'strumica',\n",
       "  269: 'forecastedcashflow',\n",
       "  270: 'bcg',\n",
       "  271: 'kalamazoo',\n",
       "  272: 're-negotiated',\n",
       "  273: 'equallogic',\n",
       "  274: 'gyro',\n",
       "  275: 'patio',\n",
       "  276: 'nue',\n",
       "  277: 'classifiero',\n",
       "  278: 'v1612',\n",
       "  279: 'southside',\n",
       "  280: 'applescript',\n",
       "  281: 'either',\n",
       "  282: 'oif',\n",
       "  283: 'recrystallization',\n",
       "  284: 'autoglas',\n",
       "  285: 'build-out',\n",
       "  286: 'grc',\n",
       "  287: 'mfr',\n",
       "  288: 'cross-database',\n",
       "  289: 'jobscatering',\n",
       "  290: 'early-warning',\n",
       "  291: 'three-person',\n",
       "  292: 'assembly',\n",
       "  293: 'insilico',\n",
       "  294: 'endodonthic',\n",
       "  295: 'testopium',\n",
       "  296: 'pander',\n",
       "  297: '2006-august',\n",
       "  298: 'igrafix',\n",
       "  299: 'thinkingjennifer',\n",
       "  300: 'policlinico',\n",
       "  301: 'usingvba',\n",
       "  302: 'testedcompound',\n",
       "  303: 'universally',\n",
       "  304: 'luggage',\n",
       "  305: 'compounds2004detection',\n",
       "  306: 'servermobile',\n",
       "  307: 'bioinformatician',\n",
       "  308: 'bdiploma',\n",
       "  309: 'education-professional',\n",
       "  310: 'galvanise',\n",
       "  311: '120mmscf',\n",
       "  312: 'web-application',\n",
       "  313: 'user-friend',\n",
       "  314: 'eev',\n",
       "  315: 'eye',\n",
       "  316: 'harmony',\n",
       "  317: 'alloy',\n",
       "  318: 'zro2',\n",
       "  319: 'liposome-like',\n",
       "  320: 'forging',\n",
       "  321: 'topotential',\n",
       "  322: 'lm',\n",
       "  323: 'polymeric',\n",
       "  324: 'brca1pedigree',\n",
       "  325: 'fmsfie',\n",
       "  326: 'spomi',\n",
       "  327: 'riccarton',\n",
       "  328: 'analystdowner',\n",
       "  329: 'elicited',\n",
       "  330: 'spanish-speaking',\n",
       "  331: 'christleton',\n",
       "  332: 'webproduct',\n",
       "  333: 'spectral',\n",
       "  334: 'viscoelastic',\n",
       "  335: '27th',\n",
       "  336: 'fluency',\n",
       "  337: '06',\n",
       "  338: 'repression',\n",
       "  339: 'councilconducted',\n",
       "  340: 'helico',\n",
       "  341: 'phenomenon',\n",
       "  342: 'telepresence',\n",
       "  343: 'electrophoresi',\n",
       "  344: 'disapprove',\n",
       "  345: 'mnm',\n",
       "  346: 'kick',\n",
       "  347: 'pharmacotherapy',\n",
       "  348: 'publicized',\n",
       "  349: 'rcf',\n",
       "  350: '5-10',\n",
       "  351: 'vbdatabasis',\n",
       "  352: 'ideal',\n",
       "  353: 'combing',\n",
       "  354: 'carrot-top',\n",
       "  355: 'hydrocollator',\n",
       "  356: 'insightmarvin-fresh',\n",
       "  357: 'sortino',\n",
       "  358: \"'test\",\n",
       "  359: 'un-necessary',\n",
       "  360: 'electricand',\n",
       "  361: 'uniliver',\n",
       "  362: 'db2environmentetl',\n",
       "  363: 'designnational',\n",
       "  364: 'girl',\n",
       "  365: 'mgmtcomtrac',\n",
       "  366: 'systemsexcellent',\n",
       "  367: 'periodclient',\n",
       "  368: 'data-scraping',\n",
       "  369: 'sqlmobile',\n",
       "  370: 'non-traditional',\n",
       "  371: 'abilities12',\n",
       "  372: 'gwa',\n",
       "  373: 'genmod',\n",
       "  374: 'oauth',\n",
       "  375: 'spence',\n",
       "  376: 'marnock',\n",
       "  377: 'trackman',\n",
       "  378: 'sdlchigh-level',\n",
       "  379: 'minipig',\n",
       "  380: 'enriched',\n",
       "  381: 'bbb',\n",
       "  382: '857',\n",
       "  383: 'non-volatile',\n",
       "  384: 'agf',\n",
       "  385: 'off-the-shelf',\n",
       "  386: 'lehigh',\n",
       "  387: 'elderlyperson',\n",
       "  388: 'plattsburgh',\n",
       "  389: 'dupont',\n",
       "  390: 'utilitydetail',\n",
       "  391: 'disbursement',\n",
       "  392: 'format',\n",
       "  393: 'dominion',\n",
       "  394: 'middleburg',\n",
       "  395: 'dispenser',\n",
       "  396: 'branding',\n",
       "  397: 'time-keeping',\n",
       "  398: 'manugistic',\n",
       "  399: 'outlook3d',\n",
       "  400: 'sparkcontext',\n",
       "  401: 'trench',\n",
       "  402: 'minimizing',\n",
       "  403: 'filtered',\n",
       "  404: 'munich',\n",
       "  405: 'countydevelopmental',\n",
       "  406: 'onnationwide',\n",
       "  407: 'denial',\n",
       "  408: 'funding',\n",
       "  409: 'costing',\n",
       "  410: 'bounded',\n",
       "  411: 'tcbb-2014-10-0438',\n",
       "  412: '2012provided',\n",
       "  413: 'specialistaccomplishment',\n",
       "  414: 'tp',\n",
       "  415: 'perfected',\n",
       "  416: 'miop',\n",
       "  417: 'repaired',\n",
       "  418: 'label-free',\n",
       "  419: 'gastonium',\n",
       "  420: 'sort-order',\n",
       "  421: 'radrequirement',\n",
       "  422: 'adjust',\n",
       "  423: 'servicecommunication',\n",
       "  424: 'ch',\n",
       "  425: 'quoted',\n",
       "  426: 'webcast',\n",
       "  427: 'happen',\n",
       "  428: 'scientist-lab',\n",
       "  429: 'settingnorton',\n",
       "  430: 'contentiou',\n",
       "  431: 'sieble',\n",
       "  432: 'skalar',\n",
       "  433: 'junit',\n",
       "  434: 'itenology',\n",
       "  435: '2016duty',\n",
       "  436: 'nslookup',\n",
       "  437: 'fed',\n",
       "  438: 'aeronautic',\n",
       "  439: 'il-12',\n",
       "  440: 'tsg101',\n",
       "  441: 'axion',\n",
       "  442: 'vectorized',\n",
       "  443: 'quarterjanuary',\n",
       "  444: 'issueswith',\n",
       "  445: 'rvt',\n",
       "  446: 'non-degree',\n",
       "  447: 'realized',\n",
       "  448: 'msc',\n",
       "  449: 'administrationcore',\n",
       "  450: 'toughest',\n",
       "  451: 'millimeter',\n",
       "  452: 'uterine',\n",
       "  453: 'simmechanic',\n",
       "  454: 'palm',\n",
       "  455: 'balsamic',\n",
       "  456: 'offeror',\n",
       "  457: 'ideology',\n",
       "  458: 'mopa',\n",
       "  459: 'durability',\n",
       "  460: 'pre-validation',\n",
       "  461: 'rxkinetix',\n",
       "  462: 'sofium',\n",
       "  463: 'citrobatcer',\n",
       "  464: 'scotwork',\n",
       "  465: 'multiplex',\n",
       "  466: 'glenview',\n",
       "  467: 'standard',\n",
       "  468: 'downloadable',\n",
       "  469: 'gregory',\n",
       "  470: 'websitejeremy',\n",
       "  471: 'goodyear',\n",
       "  472: 'mada',\n",
       "  473: 'ealing',\n",
       "  474: 'hd-esr',\n",
       "  475: 'securing',\n",
       "  476: 'bowdoin',\n",
       "  477: 'gnu',\n",
       "  478: 'newpayment',\n",
       "  479: 'mammal',\n",
       "  480: 'yulee',\n",
       "  481: 'moirastarted',\n",
       "  482: 'targetted',\n",
       "  483: 'sign-offassisted',\n",
       "  484: 'billingsmonitored',\n",
       "  485: 'kadir',\n",
       "  486: 'exercised',\n",
       "  487: 'castleton',\n",
       "  488: 'viavus',\n",
       "  489: 'truncation',\n",
       "  490: 'moreprofitable',\n",
       "  491: 'vidhyashramam',\n",
       "  492: 'propelling',\n",
       "  493: '245kg',\n",
       "  494: '704',\n",
       "  495: 'gab1',\n",
       "  496: 'stressful',\n",
       "  497: 'offloadingnew',\n",
       "  498: 'apostolica',\n",
       "  499: 'feedbackresult',\n",
       "  500: 'recourse',\n",
       "  501: 'roel',\n",
       "  502: 'presentpartner',\n",
       "  503: 'theopportunity',\n",
       "  504: 'gnp',\n",
       "  505: 'lavastorm',\n",
       "  506: 'call',\n",
       "  507: 'andvideo',\n",
       "  508: 'sympathy',\n",
       "  509: 'taiga',\n",
       "  510: 'rt',\n",
       "  511: 'discourse',\n",
       "  512: 'governed',\n",
       "  513: 'azureml',\n",
       "  514: 'isup',\n",
       "  515: 'revenue-per-click',\n",
       "  516: '2013senior',\n",
       "  517: 'helpidentify',\n",
       "  518: 'pre-cursor',\n",
       "  519: 'error-free',\n",
       "  520: 'ssc',\n",
       "  521: 'murray',\n",
       "  522: 'multi-award',\n",
       "  523: 'creator',\n",
       "  524: 'anima',\n",
       "  525: 'located',\n",
       "  526: 'co-worker',\n",
       "  527: 'mc-12',\n",
       "  528: 'rabida',\n",
       "  529: 'wy',\n",
       "  530: 'authenticity',\n",
       "  531: 'usedexpert',\n",
       "  532: 'successfully',\n",
       "  533: 'reviewing',\n",
       "  534: 'subacqueo',\n",
       "  535: 'terrace',\n",
       "  536: 'altruistic',\n",
       "  537: 'user-training',\n",
       "  538: 'programsintake',\n",
       "  539: 'larson',\n",
       "  540: 'peril',\n",
       "  541: 'onto',\n",
       "  542: 'molybdenum',\n",
       "  543: 'bsi',\n",
       "  544: 'franceproject',\n",
       "  545: '2001july',\n",
       "  546: 'vertica',\n",
       "  547: 'kluwerclustering',\n",
       "  548: 'mary',\n",
       "  549: 'mappingsystem',\n",
       "  550: 'nuke',\n",
       "  551: 'medac',\n",
       "  552: 'dreamspark',\n",
       "  553: '29',\n",
       "  554: 'com',\n",
       "  555: 'webtrend',\n",
       "  556: 'dreamweaver',\n",
       "  557: 'token',\n",
       "  558: 'gato',\n",
       "  559: 'corresponded',\n",
       "  560: 'k-8',\n",
       "  561: 'meta-analysi',\n",
       "  562: 'companiesuk',\n",
       "  563: 'pite',\n",
       "  564: 'devry',\n",
       "  565: '328',\n",
       "  566: 'panama',\n",
       "  567: 'commercially',\n",
       "  568: 'protabit',\n",
       "  569: 'devicesperformed',\n",
       "  570: 'mullin',\n",
       "  571: 'jh',\n",
       "  572: 'thrive',\n",
       "  573: 'practiceso',\n",
       "  574: 'adsorption',\n",
       "  575: 'majored',\n",
       "  576: 'unixscript',\n",
       "  577: 'midyear',\n",
       "  578: 'c3',\n",
       "  579: 'theatre-level',\n",
       "  580: 'validity',\n",
       "  581: 'ppf',\n",
       "  582: 'gapanalysi',\n",
       "  583: 'pune',\n",
       "  584: 'wastage',\n",
       "  585: 'reel',\n",
       "  586: 'partneringacquisitionsnegotiationstrade',\n",
       "  587: 'aix-marseille',\n",
       "  588: 'shaw',\n",
       "  589: 'depression',\n",
       "  590: 'afsimagevision',\n",
       "  591: 'dme',\n",
       "  592: 'convergence',\n",
       "  593: 'internetadvertiser',\n",
       "  594: 'rule-based',\n",
       "  595: 'devise',\n",
       "  596: 'reformat',\n",
       "  597: 'coenzyme',\n",
       "  598: 'deodorant',\n",
       "  599: 'malaysium',\n",
       "  600: 'analystdsg',\n",
       "  601: '7-minute',\n",
       "  602: 're-order',\n",
       "  603: 'malibu',\n",
       "  604: 'applicationrole',\n",
       "  605: 'macrosdatabasis',\n",
       "  606: 'sourcepulse',\n",
       "  607: 'assayqualification',\n",
       "  608: 'usedleadershipcommunicationorganization',\n",
       "  609: 'antiperspirancy',\n",
       "  610: 'linhardt',\n",
       "  611: 'picture-by-picture',\n",
       "  612: 'resale',\n",
       "  613: 'accessioned',\n",
       "  614: 'suggesting',\n",
       "  615: 'supporto',\n",
       "  616: 'offspring',\n",
       "  617: 'isda',\n",
       "  618: 'shoulder',\n",
       "  619: 'usresponsibility',\n",
       "  620: 'utilise',\n",
       "  621: 'marston',\n",
       "  622: 'issuednumerou',\n",
       "  623: '2017energypro',\n",
       "  624: 'lanham',\n",
       "  625: 'depositor',\n",
       "  626: 'smoking',\n",
       "  627: 'renovated',\n",
       "  628: 'linuxmethodology',\n",
       "  629: 'eua',\n",
       "  630: 'royce',\n",
       "  631: 'xrd',\n",
       "  632: 'lung',\n",
       "  633: 'revolution',\n",
       "  634: 'cherwell',\n",
       "  635: 'companycarrying',\n",
       "  636: 'topictaking',\n",
       "  637: 'maintenanceand',\n",
       "  638: 'thesis',\n",
       "  639: '70,000',\n",
       "  640: 'snapshot',\n",
       "  641: 'post-test',\n",
       "  642: 'dana',\n",
       "  643: 'comprehensively',\n",
       "  644: 'substrate',\n",
       "  645: 'wolf',\n",
       "  646: 'rscad',\n",
       "  647: 'loaf',\n",
       "  648: 'negligent',\n",
       "  649: 'list',\n",
       "  650: 'minneanalytic',\n",
       "  651: 'classified',\n",
       "  652: 'navfac',\n",
       "  653: 'screenshot',\n",
       "  654: 'warehousingrequirement',\n",
       "  655: 'chai',\n",
       "  656: 'modeller',\n",
       "  657: 'intermec',\n",
       "  658: '10khz',\n",
       "  659: 'significantfeature',\n",
       "  660: 'mongodbjira',\n",
       "  661: 'geo-referenced',\n",
       "  662: 'mciip',\n",
       "  663: '3214',\n",
       "  664: 'sap-web',\n",
       "  665: 'dischargebritish',\n",
       "  666: 'seniormanager',\n",
       "  667: 'cyberbullying',\n",
       "  668: 'brink',\n",
       "  669: 'datan',\n",
       "  670: 'toresource',\n",
       "  671: 'impacted',\n",
       "  672: 'buchanan',\n",
       "  673: 'handlingorganisation',\n",
       "  674: 'engineeringoctober',\n",
       "  675: 'levelapmg',\n",
       "  676: '2024',\n",
       "  677: 'pre-test',\n",
       "  678: 'medoption',\n",
       "  679: 'stover',\n",
       "  680: 'expiring',\n",
       "  681: 'in-clas',\n",
       "  682: 'zyxel',\n",
       "  683: 'pre-selected',\n",
       "  684: '225m',\n",
       "  685: 'passenglish',\n",
       "  686: 'nutrition',\n",
       "  687: 'rancher',\n",
       "  688: 'ews',\n",
       "  689: 'eyf',\n",
       "  690: '6-12',\n",
       "  691: 'exposurebioinformatic',\n",
       "  692: 'pearl',\n",
       "  693: 'ongoingprogres',\n",
       "  694: 'calcareou',\n",
       "  695: 'on-hand',\n",
       "  696: 'vallabhbhai',\n",
       "  697: 'facilitatorexperienced',\n",
       "  698: 'missions2',\n",
       "  699: 'infiltrator',\n",
       "  700: 'threatened',\n",
       "  701: 'costed',\n",
       "  702: 'infringement',\n",
       "  703: 'willask',\n",
       "  704: 'londonjune',\n",
       "  705: 'mosiero',\n",
       "  706: 'managementmanual',\n",
       "  707: 'neori',\n",
       "  708: 'bfm',\n",
       "  709: 'weighton',\n",
       "  710: 'lifespan',\n",
       "  711: 'batching',\n",
       "  712: 'ibioic',\n",
       "  713: 'requirement-by-requirement',\n",
       "  714: '2006epo',\n",
       "  715: 'triad',\n",
       "  716: 'alabama',\n",
       "  717: 'omniture',\n",
       "  718: 'lawsstock',\n",
       "  719: 'predictionso',\n",
       "  720: 'servicesand',\n",
       "  721: 'linked-graph',\n",
       "  722: 'dose',\n",
       "  723: 'advocacy',\n",
       "  724: 'plese',\n",
       "  725: 'trivandrum',\n",
       "  726: 'durationipvpn',\n",
       "  727: '2000jira',\n",
       "  728: 'c260',\n",
       "  729: 'proof-of-concept',\n",
       "  730: 'fiberglas',\n",
       "  731: 'detainee',\n",
       "  732: 'centerscheduler',\n",
       "  733: 'graphdatabasis',\n",
       "  734: 'preformed',\n",
       "  735: 'systemcascade',\n",
       "  736: 'documenter',\n",
       "  737: 'chinainfrastructure',\n",
       "  738: 'cornerstone',\n",
       "  739: 'sqlplu',\n",
       "  740: 'tobe',\n",
       "  741: 'photolithographyteaching',\n",
       "  742: 'upgrading',\n",
       "  743: 'bridgford',\n",
       "  744: 'universitt',\n",
       "  745: 'customerfacing',\n",
       "  746: 'nhsmail2',\n",
       "  747: 'smi',\n",
       "  748: 'delicatessen',\n",
       "  749: 'day',\n",
       "  750: \"'experience\",\n",
       "  751: 'well-organised',\n",
       "  752: 'control-mrequirement',\n",
       "  753: 'nextech',\n",
       "  754: 'gimbal',\n",
       "  755: 'relying',\n",
       "  756: 'generatestimulu',\n",
       "  757: 'teambuilt',\n",
       "  758: 'phh',\n",
       "  759: 'simplicity',\n",
       "  760: 'analysisagile',\n",
       "  761: '2015xuber',\n",
       "  762: 'cartesi',\n",
       "  763: 'meritinterestus',\n",
       "  764: 'synthetases2017neurodegenerative',\n",
       "  765: 'contacting',\n",
       "  766: 'detaileddesign',\n",
       "  767: 'building',\n",
       "  768: 'hen',\n",
       "  769: 'sastek',\n",
       "  770: 'actionline',\n",
       "  771: 'floorplan',\n",
       "  772: 'determinant',\n",
       "  773: '208',\n",
       "  774: 'detailed-oriented',\n",
       "  775: 'skillsproject',\n",
       "  776: \"company'spreliminary\",\n",
       "  777: '12k',\n",
       "  778: '2biswajit',\n",
       "  779: 'nontechnical',\n",
       "  780: '250m',\n",
       "  781: 'pastordeveloped',\n",
       "  782: 'casino',\n",
       "  783: 'dialect',\n",
       "  784: 'socializing',\n",
       "  785: 'mountbatten',\n",
       "  786: 'photographing',\n",
       "  787: 'packagei',\n",
       "  788: 'skadden',\n",
       "  789: 'inorganic',\n",
       "  790: 'educationjune',\n",
       "  791: 'clinicalfinding',\n",
       "  792: 'minewa',\n",
       "  793: 'darus',\n",
       "  794: 'rheological',\n",
       "  795: 'institut',\n",
       "  796: 'trail',\n",
       "  797: 'increment',\n",
       "  798: 'puppet',\n",
       "  799: 'isproject',\n",
       "  800: 'evidentiaryobjection',\n",
       "  801: 'progression',\n",
       "  802: 'skillsleadership',\n",
       "  803: 'dynamic',\n",
       "  804: 'ligament',\n",
       "  805: 'gromit',\n",
       "  806: '2013',\n",
       "  807: 'xpres',\n",
       "  808: 'k562',\n",
       "  809: 'wascana',\n",
       "  810: 'nh',\n",
       "  811: 'competencies-effective',\n",
       "  812: 'suspicion',\n",
       "  813: 'insiebel',\n",
       "  814: 'v2v',\n",
       "  815: 'castellini',\n",
       "  816: 'staggering',\n",
       "  817: 'uscg',\n",
       "  818: 'highest-level',\n",
       "  819: 'asa',\n",
       "  820: 'teamskill',\n",
       "  821: 'polysaccharide',\n",
       "  822: 'marketswith',\n",
       "  823: 'cardwell',\n",
       "  824: 'cardinal',\n",
       "  825: 'draftedmemoranda',\n",
       "  826: 'iberium',\n",
       "  827: '2016academic',\n",
       "  828: 'proforma',\n",
       "  829: 'brokerage',\n",
       "  830: 'java\\\\selenium',\n",
       "  831: 'designated',\n",
       "  832: 'sparx',\n",
       "  833: 'embrace',\n",
       "  834: 'amigo',\n",
       "  835: 'equip',\n",
       "  836: 'nine-month',\n",
       "  837: 'argo',\n",
       "  838: 'implementationmy',\n",
       "  839: 'maintenancetemperature',\n",
       "  840: 'bio-similar',\n",
       "  841: 'willutilise',\n",
       "  842: 'andsource',\n",
       "  843: 'tabloid',\n",
       "  844: 'crew',\n",
       "  845: 'windchill',\n",
       "  846: 'responsibilitiesdelivery',\n",
       "  847: 'peruse',\n",
       "  848: 'school',\n",
       "  849: 'lithium',\n",
       "  850: 'dataview',\n",
       "  851: 'insurancei',\n",
       "  852: 'whitley',\n",
       "  853: 'customerized',\n",
       "  854: 'kong',\n",
       "  855: 'rsc',\n",
       "  856: 'atdd',\n",
       "  857: 'sortprovider',\n",
       "  858: 'jennison',\n",
       "  859: 'powerpointsupervisor',\n",
       "  860: 'santander',\n",
       "  861: 'performedqa',\n",
       "  862: 'steering',\n",
       "  863: 'sense',\n",
       "  864: 'danny',\n",
       "  865: 'vt',\n",
       "  866: 'assignmentsmulti-taskingquick',\n",
       "  867: 'hydra',\n",
       "  868: 'biomedical',\n",
       "  869: 'next-gen',\n",
       "  870: 'conform',\n",
       "  871: 'microteaching',\n",
       "  872: 'lanchester',\n",
       "  873: 'ctp1',\n",
       "  874: 'provided',\n",
       "  875: 'quickload',\n",
       "  876: 'yamunanagar',\n",
       "  877: 'disc',\n",
       "  878: 'coded',\n",
       "  879: 'jsonmicrosoft',\n",
       "  880: 'marszaek',\n",
       "  881: 'omni',\n",
       "  882: '5000',\n",
       "  883: 'serverdefect',\n",
       "  884: 'restrictive',\n",
       "  885: 'research-advanced',\n",
       "  886: 'tmp',\n",
       "  887: '2014-datum',\n",
       "  888: 'despair',\n",
       "  889: 'leavey',\n",
       "  890: 'scjp-certified',\n",
       "  891: 'all-vs-one',\n",
       "  892: 'bodi',\n",
       "  893: 'vertex',\n",
       "  894: 'meld',\n",
       "  895: 'websense',\n",
       "  896: 'nto',\n",
       "  897: 'cloud-connected',\n",
       "  898: 'begley',\n",
       "  899: 'ecg',\n",
       "  900: 'als70',\n",
       "  901: 'self-motivator',\n",
       "  902: 'referencing',\n",
       "  903: 'prioitize',\n",
       "  904: 'adjustor',\n",
       "  905: 'sicknes',\n",
       "  906: 'criminology',\n",
       "  907: 'mosh',\n",
       "  908: 'ulf',\n",
       "  909: 'easyjet',\n",
       "  910: 'nanoscience',\n",
       "  911: 'unix-based',\n",
       "  912: 'cdk1',\n",
       "  913: 'andgoogle',\n",
       "  914: 'claimsproject',\n",
       "  915: 'iparticipated',\n",
       "  916: 'tool-worklight',\n",
       "  917: 'evs1',\n",
       "  918: 'sanitation',\n",
       "  919: 'witham',\n",
       "  920: 'dataintegration',\n",
       "  921: 'avenuenewcastle',\n",
       "  922: 'accomplishmentssurpas',\n",
       "  923: '3,400',\n",
       "  924: 'customer-orientatedcustomer',\n",
       "  925: 'cross-team',\n",
       "  926: \"i'mflexible\",\n",
       "  927: '2016finance',\n",
       "  928: '10027',\n",
       "  929: 'classic',\n",
       "  930: 'moc',\n",
       "  931: 'lucidly',\n",
       "  932: 'light-weightdocumentationo',\n",
       "  933: 'vara',\n",
       "  934: 'westlaw',\n",
       "  935: 'imagework',\n",
       "  936: 'poppelton',\n",
       "  937: '2005-2006',\n",
       "  938: 'signaller',\n",
       "  939: 'rome',\n",
       "  940: 'outlook98',\n",
       "  941: 'bandit',\n",
       "  942: 'diploma-purchasing',\n",
       "  943: 'kirtland',\n",
       "  944: 'furnishing',\n",
       "  945: 'hhsc',\n",
       "  946: 'douglas',\n",
       "  947: 'decisively',\n",
       "  948: 'peaceweek',\n",
       "  949: 'angular',\n",
       "  950: 'facet',\n",
       "  951: 'welfare',\n",
       "  952: 'alogic',\n",
       "  953: 'negociate',\n",
       "  954: 'baccalaurat',\n",
       "  955: 'casing',\n",
       "  956: 'hid',\n",
       "  957: 'evening',\n",
       "  958: 'mxd',\n",
       "  959: '2012project',\n",
       "  960: 'senescence',\n",
       "  961: '2014human',\n",
       "  962: 'scholarshipapril',\n",
       "  963: 'technicaldirection',\n",
       "  964: 'mannermentoring',\n",
       "  965: 'tsf',\n",
       "  966: 'epsilon',\n",
       "  967: 'byker',\n",
       "  968: 'edgware',\n",
       "  969: 'middlesexi',\n",
       "  970: '277',\n",
       "  971: 'circle',\n",
       "  972: 'request',\n",
       "  973: 'skype-based',\n",
       "  974: 'permanency',\n",
       "  975: 'gpio',\n",
       "  976: 'archaea',\n",
       "  977: 'iqbal',\n",
       "  978: 'initialising',\n",
       "  979: 'purposejade',\n",
       "  980: 'ambiguity',\n",
       "  981: 'msdoslanguage',\n",
       "  982: 'nahrain',\n",
       "  983: 'mathematics-b',\n",
       "  984: 'documentedbusines',\n",
       "  985: 'insalesforce',\n",
       "  986: 'realtor',\n",
       "  987: 'resubmission',\n",
       "  988: 'endangered',\n",
       "  989: 'compare',\n",
       "  990: 'ald',\n",
       "  991: 'senior-level',\n",
       "  992: 'babaie',\n",
       "  993: 'mohali',\n",
       "  994: '5tei',\n",
       "  995: 'opensoc',\n",
       "  996: 'avila',\n",
       "  997: 'bbi',\n",
       "  998: 'jone',\n",
       "  999: '707',\n",
       "  ...})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data(data_normalized,w2i=w2i,target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all = []\n",
    "if not isinstance(data_normalized, type(None)):\n",
    "    for token in data_normalized:\n",
    "        if not isinstance(token, type(None)):\n",
    "            for sentence in token:\n",
    "                if not isinstance(sentence, type(None)):\n",
    "                    for s in sentence:\n",
    "                        data_all.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freelance',\n",
       " 'copy',\n",
       " 'editor',\n",
       " 'of',\n",
       " '100',\n",
       " 'research',\n",
       " 'and',\n",
       " 'commercialisation',\n",
       " 'intern',\n",
       " 'hold',\n",
       " 'full',\n",
       " 'uk',\n",
       " 'driving',\n",
       " 'license',\n",
       " 'volunteer',\n",
       " 'campsite',\n",
       " 'attendant',\n",
       " 'light-scattering',\n",
       " 'technique',\n",
       " 'inkjet',\n",
       " 'printing',\n",
       " 'nmr',\n",
       " 'size-exclusion',\n",
       " 'chromatograhy',\n",
       " 'uv-visible',\n",
       " 'spectroscopy',\n",
       " 'fluorescence',\n",
       " 'post-doctoral',\n",
       " 'research',\n",
       " 'associate',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'night',\n",
       " 'manager',\n",
       " 'relief',\n",
       " 'manager',\n",
       " 'supervisor',\n",
       " 'duty',\n",
       " 'manager',\n",
       " 'assistant',\n",
       " 'manager',\n",
       " 'postdoctoral',\n",
       " 'research',\n",
       " 'assistant',\n",
       " 'subharmonic',\n",
       " 'function',\n",
       " 'on',\n",
       " 'graph',\n",
       " 'differential',\n",
       " 'equation',\n",
       " 'project',\n",
       " 'within',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'programming',\n",
       " 'expertise',\n",
       " 'and',\n",
       " 'consulting',\n",
       " 'delivering',\n",
       " 'result',\n",
       " 'preparing',\n",
       " 'concise',\n",
       " 'and',\n",
       " 'logically-written',\n",
       " 'material',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'organize',\n",
       " 'and',\n",
       " 'communicate',\n",
       " 'idea',\n",
       " 'effectively',\n",
       " 'in',\n",
       " 'oral',\n",
       " 'presentation',\n",
       " 'to',\n",
       " 'small',\n",
       " 'and',\n",
       " 'large',\n",
       " 'group',\n",
       " 'meeting',\n",
       " 'deadline',\n",
       " 'and',\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'the',\n",
       " 'requirement',\n",
       " 'taking',\n",
       " 'part',\n",
       " 'at',\n",
       " 'conference',\n",
       " 'belgium',\n",
       " 'france',\n",
       " 'germany',\n",
       " 'russium',\n",
       " 'etc',\n",
       " 'summer',\n",
       " 'school',\n",
       " 'austrium',\n",
       " 'usa',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'culture',\n",
       " 'and',\n",
       " 'background',\n",
       " 'skill',\n",
       " 'and',\n",
       " 'achievementsit',\n",
       " 'proficiency',\n",
       " 'and',\n",
       " 'programming',\n",
       " 'skill',\n",
       " 'matlab',\n",
       " 'creating',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'wave',\n",
       " 'propagation',\n",
       " 'on',\n",
       " 'flexural',\n",
       " 'system',\n",
       " '2015',\n",
       " 'maple',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'scientific',\n",
       " 'calculation',\n",
       " 'for',\n",
       " 'studying',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'value',\n",
       " 'property',\n",
       " 'ofsubharmonic',\n",
       " 'function',\n",
       " 'on',\n",
       " 'graph',\n",
       " 'microsoft',\n",
       " 'visual',\n",
       " 'c',\n",
       " 'creating',\n",
       " 'of',\n",
       " 'a',\n",
       " 'student',\n",
       " 'database',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'different',\n",
       " 'sorting',\n",
       " 'methodsfor',\n",
       " 'different',\n",
       " 'sequence',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'and',\n",
       " 'finite-state',\n",
       " 'automaton',\n",
       " 'simulator',\n",
       " 'numerical',\n",
       " 'solution',\n",
       " 'ofdifferential',\n",
       " 'equation',\n",
       " 'project',\n",
       " 'within',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'programming',\n",
       " 'sql',\n",
       " 'started',\n",
       " 'to',\n",
       " 'study',\n",
       " 'coursera',\n",
       " '2016',\n",
       " 'analytical',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'thinkingworking',\n",
       " 'acros',\n",
       " 'distinct',\n",
       " 'area',\n",
       " 'of',\n",
       " 'pure',\n",
       " 'and',\n",
       " 'applied',\n",
       " 'mathematics',\n",
       " 'demonstrated',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'problem',\n",
       " 'adopt',\n",
       " 'suitable',\n",
       " 'strategy',\n",
       " 'develop',\n",
       " 'new',\n",
       " 'technique',\n",
       " 'and',\n",
       " 'think',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'box',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'synthesize',\n",
       " 'large',\n",
       " 'quantity',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'datum',\n",
       " 'use',\n",
       " 'logical',\n",
       " 'argument',\n",
       " 'design',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'and',\n",
       " 'consultingcollaborating',\n",
       " 'with',\n",
       " 'engineer',\n",
       " 'on',\n",
       " 'the',\n",
       " 'resent',\n",
       " 'project',\n",
       " 'and',\n",
       " 'working',\n",
       " 'a',\n",
       " 'a',\n",
       " 'tutor',\n",
       " 'presented',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'comprehend',\n",
       " 'new',\n",
       " 'material',\n",
       " 'and',\n",
       " 'subject',\n",
       " 'matter',\n",
       " 'quickly',\n",
       " 'evaluate',\n",
       " 'and',\n",
       " 'give',\n",
       " 'a',\n",
       " 'feedback',\n",
       " 'on',\n",
       " 'the',\n",
       " 'work',\n",
       " 'teach',\n",
       " 'skill',\n",
       " 'or',\n",
       " 'concept',\n",
       " 'to',\n",
       " 'resultsa',\n",
       " 'range',\n",
       " 'of',\n",
       " 'activity',\n",
       " 'including',\n",
       " 'teaching',\n",
       " 'preparing',\n",
       " 'a',\n",
       " 'phd',\n",
       " 'thesi',\n",
       " 'research',\n",
       " 'paper',\n",
       " 'conference',\n",
       " 'presenta-tion',\n",
       " 'poster',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'written',\n",
       " 'editorial',\n",
       " 'and',\n",
       " 'public',\n",
       " 'speaking',\n",
       " 'skill',\n",
       " 'preparing',\n",
       " 'concise',\n",
       " 'and',\n",
       " 'logically-written',\n",
       " 'material',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'organize',\n",
       " 'and',\n",
       " 'communicate',\n",
       " 'idea',\n",
       " 'effectively',\n",
       " 'in',\n",
       " 'oral',\n",
       " 'presentation',\n",
       " 'to',\n",
       " 'small',\n",
       " 'and',\n",
       " 'large',\n",
       " 'group',\n",
       " 'explaining',\n",
       " 'complex',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'from',\n",
       " 'different',\n",
       " 'background',\n",
       " 'responding',\n",
       " 'appropriately',\n",
       " 'to',\n",
       " 'positive',\n",
       " 'or',\n",
       " 'negative',\n",
       " 'feedback',\n",
       " 'meeting',\n",
       " 'deadline',\n",
       " 'and',\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'the',\n",
       " 'networking',\n",
       " 'and',\n",
       " 'building',\n",
       " 'relationshipstaking',\n",
       " 'part',\n",
       " 'at',\n",
       " 'conference',\n",
       " 'belgium',\n",
       " 'france',\n",
       " 'germany',\n",
       " 'russium',\n",
       " 'etc',\n",
       " 'summer',\n",
       " 'school',\n",
       " 'austrium',\n",
       " 'usa',\n",
       " 'and',\n",
       " 'a',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'large',\n",
       " 'international',\n",
       " 'phd',\n",
       " 'programm',\n",
       " 'i',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'social',\n",
       " 'skill',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'culture',\n",
       " 'and',\n",
       " 'background',\n",
       " 'active',\n",
       " 'listening',\n",
       " 'and',\n",
       " 'seeing',\n",
       " 'the',\n",
       " 'point',\n",
       " 'building',\n",
       " 'of',\n",
       " 'fruitful',\n",
       " 'collaboration',\n",
       " 'with',\n",
       " 'colleague',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'expertise',\n",
       " 'and',\n",
       " 'follow',\n",
       " 'leadership',\n",
       " 'directive',\n",
       " 'at',\n",
       " 'appropriate',\n",
       " 'time',\n",
       " 'recently',\n",
       " 'obtained',\n",
       " 'a',\n",
       " 'phd',\n",
       " 'in',\n",
       " 'material',\n",
       " 'physic',\n",
       " 'from',\n",
       " 'university',\n",
       " 'college',\n",
       " 'london',\n",
       " 'ucl',\n",
       " 'uppsala',\n",
       " 'universitet',\n",
       " 'and',\n",
       " 'achieved',\n",
       " 'a',\n",
       " 'first',\n",
       " 'clas',\n",
       " 'honour',\n",
       " 'in',\n",
       " 'mscus',\n",
       " 'chemistry',\n",
       " 'from',\n",
       " 'ucl',\n",
       " 'i',\n",
       " 'have',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'laboratory',\n",
       " 'technique',\n",
       " 'computer',\n",
       " 'programming',\n",
       " 'teaching',\n",
       " 'public',\n",
       " 'engagement',\n",
       " 'international',\n",
       " 'research',\n",
       " 'collaboration',\n",
       " 'and',\n",
       " 'publication',\n",
       " 'i',\n",
       " 'am',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'incorporate',\n",
       " 'strong',\n",
       " 'scientific',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'experience',\n",
       " 'into',\n",
       " 'a',\n",
       " 'material',\n",
       " 'science-related',\n",
       " 'industry',\n",
       " 'setting',\n",
       " 'work',\n",
       " 'experience',\n",
       " 'personal',\n",
       " 'assistant',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'postgraduate',\n",
       " 'research',\n",
       " 'student',\n",
       " 'who',\n",
       " 'is',\n",
       " 'currently',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'work',\n",
       " 'opportunity',\n",
       " 'whilst',\n",
       " 'studying',\n",
       " 'i',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ambitiou',\n",
       " 'person',\n",
       " 'who',\n",
       " 'ha',\n",
       " 'alway',\n",
       " 'taken',\n",
       " 'a',\n",
       " 'professional',\n",
       " 'and',\n",
       " 'responsible',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'any',\n",
       " 'task',\n",
       " 'that',\n",
       " 'i',\n",
       " 'have',\n",
       " 'undertaken',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'teaching',\n",
       " 'part-time',\n",
       " 'on',\n",
       " 'science',\n",
       " 'undergraduate',\n",
       " 'course',\n",
       " 'along',\n",
       " 'with',\n",
       " 'demonstrating',\n",
       " 'technical',\n",
       " 'lab',\n",
       " 'skill',\n",
       " 'molecular',\n",
       " 'biologist',\n",
       " 'employability',\n",
       " 'assistant',\n",
       " 'postgraduate',\n",
       " 'student',\n",
       " 'representative',\n",
       " 'school',\n",
       " 'of',\n",
       " 'proces',\n",
       " 'and',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'postgraduate',\n",
       " 'student',\n",
       " 'representative',\n",
       " 'school',\n",
       " 'of',\n",
       " 'proces',\n",
       " 'and',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'sale',\n",
       " 'assistant',\n",
       " 'social',\n",
       " 'secretary',\n",
       " 'post-doctoral',\n",
       " 'research',\n",
       " 'associate',\n",
       " 'bank',\n",
       " 'deposit',\n",
       " 'procedure',\n",
       " 'health',\n",
       " 'and',\n",
       " 'safety',\n",
       " 'regulation',\n",
       " 'course',\n",
       " 'skill',\n",
       " 'excellent',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'skill',\n",
       " 'personnel',\n",
       " 'development',\n",
       " 'merchandising',\n",
       " 'bank',\n",
       " 'deposit',\n",
       " 'procedure',\n",
       " 'staff',\n",
       " 'training',\n",
       " 'and',\n",
       " 'development',\n",
       " 'opening',\n",
       " 'and',\n",
       " 'closing',\n",
       " 'procedure',\n",
       " 'basic',\n",
       " 'food',\n",
       " 'hygiene',\n",
       " 'certificate',\n",
       " 'health',\n",
       " 'and',\n",
       " 'safety',\n",
       " 'regulation',\n",
       " 'course',\n",
       " 'hospitality',\n",
       " 'course',\n",
       " 'national',\n",
       " 'license',\n",
       " 'certificate',\n",
       " 'local',\n",
       " 'organizer',\n",
       " 'of',\n",
       " 'a',\n",
       " 'workshop',\n",
       " 'teaching',\n",
       " 'assistant',\n",
       " 'skill',\n",
       " 'and',\n",
       " 'achievement',\n",
       " 'microsoft',\n",
       " 'visual',\n",
       " 'c',\n",
       " 'creating',\n",
       " 'of',\n",
       " 'a',\n",
       " 'student',\n",
       " 'database',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'different',\n",
       " 'sorting',\n",
       " 'method',\n",
       " 'analyze',\n",
       " 'problem',\n",
       " 'design',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'survey',\n",
       " 'comprehend',\n",
       " 'new',\n",
       " 'material',\n",
       " 'and',\n",
       " 'subject',\n",
       " 'matter',\n",
       " 'quickly',\n",
       " 'teach',\n",
       " 'skill',\n",
       " 'or',\n",
       " 'concept',\n",
       " 'to',\n",
       " 'other',\n",
       " 'explaining',\n",
       " 'complex',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'from',\n",
       " 'different',\n",
       " 'background',\n",
       " 'communication',\n",
       " 'networking',\n",
       " 'and',\n",
       " 'building',\n",
       " 'relationship',\n",
       " 'gcse',\n",
       " 'science',\n",
       " 'teacher',\n",
       " 'saturday',\n",
       " 'school',\n",
       " 'chemical',\n",
       " 'engineer',\n",
       " 'consultant',\n",
       " 'clinical',\n",
       " 'psychologist',\n",
       " 'bartender',\n",
       " 'waitres',\n",
       " 'i',\n",
       " 'consider',\n",
       " 'myself',\n",
       " 'to',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'hard-working',\n",
       " 'hand',\n",
       " 'on',\n",
       " 'and',\n",
       " 'a',\n",
       " 'reliable',\n",
       " 'worker',\n",
       " 'and',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " 'keeper',\n",
       " 'i',\n",
       " 'have',\n",
       " 'good',\n",
       " 'communication',\n",
       " 'skill',\n",
       " 'i',\n",
       " 'am',\n",
       " 'very',\n",
       " 'confident',\n",
       " 'when',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'other',\n",
       " 'and',\n",
       " 'can',\n",
       " 'work',\n",
       " 'well',\n",
       " 'either',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'team',\n",
       " 'and',\n",
       " 'alone',\n",
       " 'using',\n",
       " 'my',\n",
       " 'own',\n",
       " 'initiative',\n",
       " 'i',\n",
       " 'welcome',\n",
       " 'new',\n",
       " 'challenge',\n",
       " 'with',\n",
       " 'open',\n",
       " 'arm',\n",
       " 'and',\n",
       " 'am',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'undertake',\n",
       " 'further',\n",
       " 'training',\n",
       " 'a',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'enhance',\n",
       " 'my',\n",
       " 'career',\n",
       " 'have',\n",
       " 'extensive',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'and',\n",
       " 'generally',\n",
       " 'enjoy',\n",
       " 'helping',\n",
       " 'person',\n",
       " 'the',\n",
       " 'social',\n",
       " 'skill',\n",
       " 'i',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'through',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'work',\n",
       " 'and',\n",
       " 'socializing',\n",
       " 'with',\n",
       " 'other',\n",
       " 'inside',\n",
       " 'and',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'the',\n",
       " 'workplace',\n",
       " 'have',\n",
       " 'shaped',\n",
       " 'me',\n",
       " 'into',\n",
       " 'a',\n",
       " 'confident',\n",
       " 'outgoing',\n",
       " 'and',\n",
       " 'adaptable',\n",
       " 'employee',\n",
       " 'and',\n",
       " 'i',\n",
       " 'believe',\n",
       " 'i',\n",
       " 'can',\n",
       " 'transfer',\n",
       " 'and',\n",
       " 'apply',\n",
       " 'the',\n",
       " 'skill',\n",
       " 'and',\n",
       " 'knowledge',\n",
       " 'to',\n",
       " 'any',\n",
       " 'position',\n",
       " 'i',\n",
       " 'aim',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'freelance',\n",
       " 'graphic',\n",
       " 'and',\n",
       " 'page',\n",
       " 'designer',\n",
       " 'french',\n",
       " 'basic',\n",
       " 'adobe',\n",
       " 'creative',\n",
       " 'suite',\n",
       " 'cs',\n",
       " 'software',\n",
       " 'lecturer',\n",
       " 'in',\n",
       " 'international',\n",
       " 'busines',\n",
       " 'busines',\n",
       " 'economic',\n",
       " 'doctoral',\n",
       " 'researcher',\n",
       " 'doctoral',\n",
       " 'researcher',\n",
       " 'skill',\n",
       " 'expertise',\n",
       " 'warehouse',\n",
       " 'operative',\n",
       " 'support',\n",
       " 'worker',\n",
       " 'part-time',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'recent',\n",
       " 'phd',\n",
       " 'graduate',\n",
       " 'in',\n",
       " 'electrical',\n",
       " 'and',\n",
       " 'electronic',\n",
       " 'engineering',\n",
       " 'and',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'job',\n",
       " 'my',\n",
       " 'research',\n",
       " 'wa',\n",
       " 'on',\n",
       " 'video',\n",
       " 'coding',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'gaze',\n",
       " 'location',\n",
       " 'prediction',\n",
       " 'during',\n",
       " 'i',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'solid',\n",
       " 'skill',\n",
       " 'on',\n",
       " 'image',\n",
       " 'processing',\n",
       " 'video',\n",
       " 'compression',\n",
       " 'statistical',\n",
       " 'analysi',\n",
       " 'and',\n",
       " 'programming',\n",
       " 'my',\n",
       " 'time',\n",
       " 'management',\n",
       " 'problem',\n",
       " 'solving',\n",
       " 'and',\n",
       " 'communication',\n",
       " 'skill',\n",
       " 'have',\n",
       " 'been',\n",
       " 'practised',\n",
       " 'i',\n",
       " 'have',\n",
       " 'achieved',\n",
       " 'first',\n",
       " 'clas',\n",
       " 'during',\n",
       " 'education',\n",
       " 'and',\n",
       " 'got',\n",
       " 'the',\n",
       " 'best',\n",
       " 'publication',\n",
       " 'in',\n",
       " 'my',\n",
       " 'phd',\n",
       " 'field',\n",
       " 'currently',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'field',\n",
       " 'like',\n",
       " 'pattern',\n",
       " 'recognition',\n",
       " 'datum',\n",
       " 'analysi',\n",
       " 'real-time',\n",
       " 'programming',\n",
       " 'and',\n",
       " 'software',\n",
       " 'engineering',\n",
       " 'i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'job',\n",
       " 'in',\n",
       " 'the',\n",
       " 'industry',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'start',\n",
       " 'of',\n",
       " 'my',\n",
       " 'career',\n",
       " 'self',\n",
       " 'employed',\n",
       " 'web',\n",
       " 'designer',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'final',\n",
       " 'stage',\n",
       " 'of',\n",
       " 'a',\n",
       " 'phd',\n",
       " 'in',\n",
       " 'polymer',\n",
       " 'chemistry',\n",
       " 'which',\n",
       " 'involved',\n",
       " 'the',\n",
       " 'formulation',\n",
       " 'synthesi',\n",
       " 'and',\n",
       " 'analysi',\n",
       " 'of',\n",
       " 'several',\n",
       " 'elastomeric',\n",
       " 'species',\n",
       " 'including',\n",
       " 'polyacrylate',\n",
       " 'polyurethane',\n",
       " 'and',\n",
       " 'styrene-butadiene',\n",
       " 'rubber',\n",
       " 'i',\n",
       " 'have',\n",
       " 'found',\n",
       " 'thi',\n",
       " 'degree',\n",
       " 'course',\n",
       " 'both',\n",
       " 'challenging',\n",
       " 'and',\n",
       " 'rewarding',\n",
       " 'a',\n",
       " 'i',\n",
       " 'am',\n",
       " 'keen',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'new',\n",
       " 'skill',\n",
       " 'however',\n",
       " 'i',\n",
       " 'have',\n",
       " 'realised',\n",
       " 'that',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'pursue',\n",
       " 'a',\n",
       " 'laboratory',\n",
       " 'based',\n",
       " 'career',\n",
       " 'in',\n",
       " 'chemistry',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'academium',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'in',\n",
       " 'the',\n",
       " 'research',\n",
       " 'and',\n",
       " 'development',\n",
       " 'of',\n",
       " 'polymeric',\n",
       " 'material',\n",
       " 'litigation',\n",
       " 'executive',\n",
       " ...]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sdasd', 'asdasd', 'sdasdas']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '''sdasd asdasd sdasdas'''\n",
    "\n",
    "s = a.split()\n",
    "\n",
    "s = ['<s>'] + s + ['</s>']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def build_vocab(data_word_tokenized, target):\n",
    "#     vocab = set()\n",
    "#     for token in data_word_tokenized:\n",
    "#         for sent in token:\n",
    "#             if not isinstance(sent, type(None)):\n",
    "#                 for word in sent:\n",
    "#                     vocab.add(word)\n",
    "            \n",
    "#     if target:\n",
    "#         w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "#         w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "#     else:\n",
    "#         w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "#     return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: map(lambda w: nltk.stem.PorterStemmer.NLTK_EXTENSIONS(w) if w in wl and not isinstance(w, type(None)) else wl.remove(w), wl), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-7f1cfeb94f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-82eefccd2ba1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_word_tokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in data_normalized:\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                for l in k:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeNoneTypes(lst):\n",
    "    return [i for i in lst if type(i) is not type(None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(data_word_tokenized, **kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    \n",
    "    data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: removeNoneTypes(wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda w: w.singularize(), l))\n",
    "    \n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    " \n",
    "     # remove tokens with length 1\n",
    "#     ree = re.compile(r'(\\'\\w)')\n",
    "#     rlen = len(data_normalized)\n",
    "#     tmp = data_normalized.copy()\n",
    "# #     tmp.reset_index(drop=True)\n",
    "#     for indx in tmp.index:\n",
    "         wl_coll = list()\n",
    "         for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "            for j in i:\n",
    "                if not isinstance(j, type(None)):\n",
    "                    for k in j:\n",
    "                        if not isinstance(k, type(None)):\n",
    "                            if re.match(ree, i):\n",
    "                                ree.sub('', i)\n",
    "                            if len(i.strip().strip('.').strip(',')) > 1:\n",
    "                                wl.append((i))\n",
    "                    wl_coll.append(WordList(wl))\n",
    "            data_normalized[indx] = wl_col\n",
    "            del tmp\n",
    "\n",
    "    # stemming\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: nltk.stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "\n",
    "    data_word_tokenized= tokenize_words('value_char', **tokenizer_prefs)\n",
    "    \n",
    "    return data_word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "tokenizer  =   RegexpTokenizer(pattern=r'\\w+')\n",
    "stemmer    =   nltk.stem.PorterStemmer.NLTK_EXTENSIONS\n",
    "lemmatize  =   nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <map object at 0x7fbaa7e44550>\n",
       "1        <map object at 0x7fbaa7e445c0>\n",
       "2        <map object at 0x7fbaa7e44630>\n",
       "3        <map object at 0x7fbaa7e446d8>\n",
       "4        <map object at 0x7fbaa7e44780>\n",
       "6        <map object at 0x7fbaa7e44828>\n",
       "7        <map object at 0x7fbaa7e448d0>\n",
       "8        <map object at 0x7fbaa7e44978>\n",
       "9        <map object at 0x7fbaa7e44a20>\n",
       "10       <map object at 0x7fbaa7e44ac8>\n",
       "11       <map object at 0x7fbaa7e44b70>\n",
       "12       <map object at 0x7fbaa7e44c18>\n",
       "13       <map object at 0x7fbaa7e44cc0>\n",
       "14       <map object at 0x7fbaa7e44d68>\n",
       "18       <map object at 0x7fbaa7e44e10>\n",
       "19       <map object at 0x7fbaa7e44eb8>\n",
       "21       <map object at 0x7fbaa7e44f60>\n",
       "22       <map object at 0x7fbaa7e46048>\n",
       "23       <map object at 0x7fbaa7e460f0>\n",
       "24       <map object at 0x7fbaa7e46198>\n",
       "25       <map object at 0x7fbaa7e46240>\n",
       "29       <map object at 0x7fbaa7e462e8>\n",
       "30       <map object at 0x7fbaa7e46390>\n",
       "31       <map object at 0x7fbaa7e46438>\n",
       "32       <map object at 0x7fbaa7e464e0>\n",
       "33       <map object at 0x7fbaa7e46588>\n",
       "34       <map object at 0x7fbaa7e46630>\n",
       "35       <map object at 0x7fbaa7e466d8>\n",
       "36       <map object at 0x7fbaa7e46780>\n",
       "37       <map object at 0x7fbaa7e46828>\n",
       "                      ...              \n",
       "26744    <map object at 0x7fbaa778ceb8>\n",
       "26745    <map object at 0x7fbaa778cf60>\n",
       "26746    <map object at 0x7fbaa778e048>\n",
       "26747    <map object at 0x7fbaa778e0f0>\n",
       "26748    <map object at 0x7fbaa778e198>\n",
       "26749    <map object at 0x7fbaa778e240>\n",
       "26751    <map object at 0x7fbaa778e2e8>\n",
       "26752    <map object at 0x7fbaa778e390>\n",
       "26753    <map object at 0x7fbaa778e438>\n",
       "26754    <map object at 0x7fbaa778e4e0>\n",
       "26755    <map object at 0x7fbaa778e588>\n",
       "26756    <map object at 0x7fbaa778e630>\n",
       "26757    <map object at 0x7fbaa778e6d8>\n",
       "26758    <map object at 0x7fbaa778e780>\n",
       "26759    <map object at 0x7fbaa778e828>\n",
       "26760    <map object at 0x7fbaa778e8d0>\n",
       "26761    <map object at 0x7fbaa778e978>\n",
       "26762    <map object at 0x7fbaa778ea20>\n",
       "26763    <map object at 0x7fbaa778eac8>\n",
       "26764    <map object at 0x7fbaa778eb70>\n",
       "26765    <map object at 0x7fbaa778ec18>\n",
       "26767    <map object at 0x7fbaa778ecc0>\n",
       "26768    <map object at 0x7fbaa778ed68>\n",
       "26769    <map object at 0x7fbaa778ee10>\n",
       "26770    <map object at 0x7fbaa778eeb8>\n",
       "26771    <map object at 0x7fbaa778ef60>\n",
       "26772    <map object at 0x7fbaa7790048>\n",
       "26773    <map object at 0x7fbaa77900f0>\n",
       "26774    <map object at 0x7fbaa7790198>\n",
       "26775    <map object at 0x7fbaa7790240>\n",
       "Name: value_char, Length: 22845, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(data_word_tokenized,**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mitcham',\n",
       " 'guest',\n",
       " 'lecturer',\n",
       " 'crewe',\n",
       " 'pencari',\n",
       " 'kerja',\n",
       " 'an',\n",
       " 'ambitious',\n",
       " 'and',\n",
       " 'hardworking',\n",
       " 'individual',\n",
       " 'who',\n",
       " 'is',\n",
       " 'motivated',\n",
       " 'by',\n",
       " 'challenge',\n",
       " 'and',\n",
       " 'is',\n",
       " 'passionate',\n",
       " 'to',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'inventor',\n",
       " 'on',\n",
       " 'four',\n",
       " 'patents',\n",
       " 'excellent',\n",
       " 'communicator',\n",
       " 'strong',\n",
       " 'planning',\n",
       " 'organisational',\n",
       " 'problem',\n",
       " 'solving',\n",
       " 'skills',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'successfully',\n",
       " 'analyse',\n",
       " 'and',\n",
       " 'assimilate',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'disparate',\n",
       " 'information',\n",
       " 'good',\n",
       " 'time',\n",
       " 'management',\n",
       " 'enjoys',\n",
       " 'working',\n",
       " 'under',\n",
       " 'pressure',\n",
       " 'and',\n",
       " 'to',\n",
       " 'deadlines',\n",
       " 'either',\n",
       " 'individually',\n",
       " 'or',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'team',\n",
       " 'conversational',\n",
       " 'german',\n",
       " 'part',\n",
       " 'time',\n",
       " 'lecturing',\n",
       " 'and',\n",
       " 'lab',\n",
       " 'demonstrator',\n",
       " 'project',\n",
       " 'evaluation',\n",
       " 'and',\n",
       " 'responsible',\n",
       " 'innovation',\n",
       " 'intern',\n",
       " 'computer',\n",
       " 'skills-',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'in',\n",
       " 'using',\n",
       " 'all',\n",
       " 'office',\n",
       " 'full',\n",
       " 'uk',\n",
       " 'driving',\n",
       " 'license',\n",
       " 'dewsbury',\n",
       " 'research',\n",
       " 'intern',\n",
       " 'micro',\n",
       " 'bio',\n",
       " 'nanofluidics',\n",
       " 'unit',\n",
       " 'attendant',\n",
       " 'lab',\n",
       " 'skills',\n",
       " 'spectroscopy',\n",
       " 'infra-red',\n",
       " 'spectroscopy',\n",
       " 'surface-tensiometry',\n",
       " 'conductivity',\n",
       " 'electron',\n",
       " 'microscopy',\n",
       " 'polymerisation',\n",
       " 'techniques',\n",
       " 'inc.',\n",
       " 'emulsion',\n",
       " 'dispersion',\n",
       " 'polymerisation',\n",
       " 'rheology',\n",
       " 'micro-piv',\n",
       " 'microfluidics',\n",
       " 'skillslight-scattering',\n",
       " 'techniques',\n",
       " 'inkjet',\n",
       " 'printing',\n",
       " 'nmr',\n",
       " 'size-exclusion',\n",
       " 'chromatograhy',\n",
       " 'uv-visible',\n",
       " 'spectroscopy',\n",
       " 'fluorescencelab',\n",
       " 'skills',\n",
       " 'spectroscopy',\n",
       " 'infra-red',\n",
       " 'spectroscopy',\n",
       " 'surface-tensiometry',\n",
       " 'conductivity',\n",
       " 'electron',\n",
       " 'microscopy',\n",
       " 'polymerisation',\n",
       " 'techniques',\n",
       " 'inc.',\n",
       " 'emulsion',\n",
       " 'dispersion',\n",
       " 'polymerisation',\n",
       " 'rheology',\n",
       " 'micro-piv',\n",
       " 'microfluidicscomputer',\n",
       " 'ms',\n",
       " 'office',\n",
       " 'ltex',\n",
       " 'inkscape',\n",
       " 'image',\n",
       " 'software',\n",
       " 'imagej',\n",
       " 'chembiooffice',\n",
       " 'origin',\n",
       " 'and',\n",
       " 'blender',\n",
       " 'basic',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'night',\n",
       " 'manager',\n",
       " 'london',\n",
       " 'excellent',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'skills',\n",
       " 'personnel',\n",
       " 'development',\n",
       " 'merchandising',\n",
       " 'staff',\n",
       " 'training',\n",
       " 'and',\n",
       " 'development',\n",
       " 'liverpool',\n",
       " 'postdoctoral',\n",
       " 'research',\n",
       " 'assistant',\n",
       " 'it',\n",
       " 'proficiency',\n",
       " 'and',\n",
       " 'programming',\n",
       " 'skills',\n",
       " 'matlab',\n",
       " 'creating',\n",
       " 'simulations',\n",
       " 'of',\n",
       " 'wave',\n",
       " 'propagation',\n",
       " 'on',\n",
       " 'flexural',\n",
       " 'systems',\n",
       " '2015',\n",
       " 'for',\n",
       " 'different',\n",
       " 'sequences',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'and',\n",
       " 'finite-state',\n",
       " 'automaton',\n",
       " 'simulators',\n",
       " 'numerical',\n",
       " 'solution',\n",
       " 'of',\n",
       " 'sql',\n",
       " 'started',\n",
       " 'to',\n",
       " 'study',\n",
       " 'coursera',\n",
       " '2016-',\n",
       " 'analytical',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'thinking',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'synthesize',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'data',\n",
       " 'range',\n",
       " 'of',\n",
       " 'activities',\n",
       " 'including',\n",
       " 'teaching',\n",
       " 'preparing',\n",
       " 'phd',\n",
       " 'thesis',\n",
       " 'research',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'presenta-',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'expertise',\n",
       " 'and',\n",
       " 'follow',\n",
       " 'leadership',\n",
       " 'directives',\n",
       " 'at',\n",
       " 'appropriate',\n",
       " 'times',\n",
       " 'marple',\n",
       " 'cheshire',\n",
       " 'uk',\n",
       " 'chemical',\n",
       " 'engineer',\n",
       " 'with',\n",
       " 'significant',\n",
       " 'experience',\n",
       " 'of',\n",
       " 'working',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'team',\n",
       " 'and',\n",
       " 'individually',\n",
       " 'in',\n",
       " 'timeline',\n",
       " 'driven',\n",
       " 'highly',\n",
       " 'pressurised',\n",
       " 'industrial',\n",
       " 'and',\n",
       " 'academic',\n",
       " 'environments',\n",
       " 'this',\n",
       " 'experience',\n",
       " 'has',\n",
       " 'been',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'large',\n",
       " 'range',\n",
       " 'of',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'science',\n",
       " 'and',\n",
       " 'biotechnology',\n",
       " 'following',\n",
       " 'several',\n",
       " 'bachelor',\n",
       " 'degrees',\n",
       " 'and',\n",
       " 'phd',\n",
       " 'in',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'this',\n",
       " 'has',\n",
       " 'manifested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'two',\n",
       " 'patent',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'which',\n",
       " 'am',\n",
       " 'co-inventor',\n",
       " 'this',\n",
       " 'industrial',\n",
       " 'experience',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'postgraduate',\n",
       " 'training',\n",
       " 'has',\n",
       " 'been',\n",
       " 'in',\n",
       " 'multi-disciplinary',\n",
       " 'environments',\n",
       " 'enabling',\n",
       " 'effective',\n",
       " 'communication',\n",
       " 'with',\n",
       " 'people',\n",
       " 'from',\n",
       " 'very',\n",
       " 'different',\n",
       " 'technical',\n",
       " 'and',\n",
       " 'non-technical',\n",
       " 'backgrounds',\n",
       " 'also',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'applying',\n",
       " 'for',\n",
       " 'chartership',\n",
       " 'london',\n",
       " 'registered',\n",
       " 'manager',\n",
       " 'london',\n",
       " 'demonstrator',\n",
       " 'chess',\n",
       " 'teacher',\n",
       " 'london',\n",
       " 'persian',\n",
       " 'native',\n",
       " 'general',\n",
       " 'operation',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'benfleet',\n",
       " 'matching',\n",
       " 'programming',\n",
       " 'matlab',\n",
       " 'scilab',\n",
       " 'shell',\n",
       " 'script',\n",
       " 'proficient',\n",
       " 'in',\n",
       " 'the',\n",
       " 'development',\n",
       " 'in',\n",
       " 'windows',\n",
       " 'and',\n",
       " 'linux',\n",
       " 'eigen',\n",
       " 'boost',\n",
       " 'opengl',\n",
       " 'etc',\n",
       " 'principal',\n",
       " 'scientist',\n",
       " 'sheffield',\n",
       " 'laboratory',\n",
       " 'demonstrator',\n",
       " 'legal',\n",
       " 'researcher',\n",
       " 'freelance',\n",
       " 'writer',\n",
       " 'all',\n",
       " 'areas',\n",
       " 'trainee',\n",
       " 'paralegal',\n",
       " 'have',\n",
       " 'excellent',\n",
       " 'skills',\n",
       " 'gained',\n",
       " 'through',\n",
       " 'regularly',\n",
       " 'using',\n",
       " 'window',\n",
       " 'based',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'microsoft',\n",
       " 'word',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'microsoft',\n",
       " 'powerpoint',\n",
       " 'computer',\n",
       " 'and',\n",
       " 'language',\n",
       " 'skills',\n",
       " 'have',\n",
       " 'excellent',\n",
       " 'skills',\n",
       " 'gained',\n",
       " 'through',\n",
       " 'regularly',\n",
       " 'using',\n",
       " 'window',\n",
       " 'based',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'microsoft',\n",
       " 'word',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'microsoft',\n",
       " 'powerpoint',\n",
       " 'have',\n",
       " 'obtained',\n",
       " 'certificate',\n",
       " 'in',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'at',\n",
       " 'level',\n",
       " 'can',\n",
       " 'read',\n",
       " 'speak',\n",
       " 'and',\n",
       " 'write',\n",
       " 'in',\n",
       " 'french',\n",
       " 'at',\n",
       " 'an',\n",
       " 'advanced',\n",
       " 'level',\n",
       " 'can',\n",
       " 'speak',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'tongue',\n",
       " 'language',\n",
       " 'punjabi',\n",
       " 'fluently',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'activities',\n",
       " 'am',\n",
       " 'and',\n",
       " 'enthusiastic',\n",
       " 'runner',\n",
       " 'am',\n",
       " 'greatly',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'mountain',\n",
       " 'climbing',\n",
       " 'as',\n",
       " 'recenlty',\n",
       " 'discovered',\n",
       " 'when',\n",
       " 'climbing',\n",
       " 'mount',\n",
       " 'sinai',\n",
       " 'in',\n",
       " 'north',\n",
       " 'africa',\n",
       " 'like',\n",
       " 'travelling',\n",
       " 'and',\n",
       " 'have',\n",
       " 'recently',\n",
       " 'been',\n",
       " 'backpacking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'artic',\n",
       " 'circle',\n",
       " 'languages',\n",
       " 'and',\n",
       " 'different',\n",
       " 'cultures',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'greatly',\n",
       " 'am',\n",
       " 'first',\n",
       " 'aider',\n",
       " 'hold',\n",
       " 'full',\n",
       " 'clean',\n",
       " 'driving',\n",
       " 'like',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'to',\n",
       " 'date',\n",
       " 'with',\n",
       " 'current',\n",
       " 'affairs',\n",
       " 'and',\n",
       " 'legal',\n",
       " 'issues',\n",
       " 'birmingham',\n",
       " 'consultant',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'developer',\n",
       " 'credit',\n",
       " 'risk',\n",
       " 'analyst',\n",
       " 'lavastorm',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'with',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'tools',\n",
       " 'such',\n",
       " 'as',\n",
       " 'qlikview',\n",
       " 'tableau',\n",
       " 'and',\n",
       " 'omniscope',\n",
       " 'cheltenham',\n",
       " 'adel',\n",
       " 'ia',\n",
       " 'marketing',\n",
       " 'manager',\n",
       " 'emea',\n",
       " 'norwich',\n",
       " 'associate',\n",
       " 'producer',\n",
       " 'production',\n",
       " 'manager',\n",
       " 'art',\n",
       " 'coordinator',\n",
       " 'associate',\n",
       " 'tutor',\n",
       " 'exam',\n",
       " 'invigilation',\n",
       " 'case',\n",
       " 'manager',\n",
       " 'job',\n",
       " 'seeker',\n",
       " 'member',\n",
       " 'of',\n",
       " 'school',\n",
       " 'executive',\n",
       " 'nottingham',\n",
       " 'talent',\n",
       " 'match',\n",
       " 'sheffield',\n",
       " 'city',\n",
       " 'region',\n",
       " 'programme',\n",
       " 'administrator',\n",
       " 'american',\n",
       " 'fork',\n",
       " 'ut',\n",
       " 'independent',\n",
       " 'consultant',\n",
       " 'global',\n",
       " 'health',\n",
       " 'sector',\n",
       " 'capacity',\n",
       " 'builder',\n",
       " 'lecturer',\n",
       " 'in',\n",
       " 'project',\n",
       " 'management',\n",
       " 'creative',\n",
       " 'and',\n",
       " 'well-rounded',\n",
       " 'graduate',\n",
       " 'with',\n",
       " 'first',\n",
       " 'class',\n",
       " 'honours',\n",
       " 'degree',\n",
       " 'in',\n",
       " 'english',\n",
       " 'and',\n",
       " 'creativewriting',\n",
       " 'and',\n",
       " 'masters',\n",
       " 'in',\n",
       " 'medical',\n",
       " 'humanities',\n",
       " 'currently',\n",
       " 'completing',\n",
       " 'phd',\n",
       " 'significant',\n",
       " 'experienceof',\n",
       " 'managing',\n",
       " 'and',\n",
       " 'motivating',\n",
       " 'large',\n",
       " 'teams',\n",
       " 'in',\n",
       " 'the',\n",
       " 'volunteering',\n",
       " 'and',\n",
       " 'charitable',\n",
       " 'sectors',\n",
       " 'exceptionalknowledge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'social',\n",
       " 'cultural',\n",
       " 'legislative',\n",
       " 'and',\n",
       " 'personal',\n",
       " 'issues',\n",
       " 'those',\n",
       " 'with',\n",
       " 'disabilities',\n",
       " 'face',\n",
       " 'attentive',\n",
       " 'todetail',\n",
       " 'with',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'freelance',\n",
       " 'editing',\n",
       " 'and',\n",
       " 'copywriting',\n",
       " 'confident',\n",
       " 'and',\n",
       " 'enthusiastic',\n",
       " 'with',\n",
       " 'excellentcommunication',\n",
       " 'skills',\n",
       " 'natural',\n",
       " 'leader',\n",
       " 'and',\n",
       " 'hands',\n",
       " 'on',\n",
       " 'team',\n",
       " 'worker',\n",
       " 'who',\n",
       " 'is',\n",
       " 'effective',\n",
       " 'at',\n",
       " 'time',\n",
       " 'managementand',\n",
       " 'presentations',\n",
       " 'to',\n",
       " 'groups',\n",
       " 'thrives',\n",
       " 'on',\n",
       " 'new',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'goals',\n",
       " 'part',\n",
       " 'time',\n",
       " 'lecturing',\n",
       " 'and',\n",
       " 'lab',\n",
       " 'demonstrator',\n",
       " 'leicestershire',\n",
       " 'uk',\n",
       " 'natural',\n",
       " 'products',\n",
       " 'research',\n",
       " 'assistant',\n",
       " 'computer',\n",
       " 'skills-',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'in',\n",
       " 'using',\n",
       " 'all',\n",
       " 'office',\n",
       " 'applications',\n",
       " 'volunteer',\n",
       " 'attendant',\n",
       " 'computer',\n",
       " 'ms',\n",
       " 'office',\n",
       " 'ltex',\n",
       " 'inkscape',\n",
       " 'image',\n",
       " 'software',\n",
       " 'imagej',\n",
       " 'chembiooffice',\n",
       " 'origin',\n",
       " 'and',\n",
       " 'blender',\n",
       " 'basic',\n",
       " 'bolton',\n",
       " 'supervisor',\n",
       " 'duty',\n",
       " 'manager',\n",
       " 'opening',\n",
       " 'and',\n",
       " 'closing',\n",
       " 'procedures',\n",
       " 'basic',\n",
       " 'food',\n",
       " 'hygiene',\n",
       " 'certificate',\n",
       " 'hospitality',\n",
       " 'course',\n",
       " 'national',\n",
       " 'license',\n",
       " 'certificate',\n",
       " 'phd',\n",
       " 'graduate',\n",
       " 'in',\n",
       " 'mathematics',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'position',\n",
       " 'in',\n",
       " 'global',\n",
       " 'company',\n",
       " 'that',\n",
       " 'works',\n",
       " 'on',\n",
       " 'real-worldproblems',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'use',\n",
       " 'my',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'expertise',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'significantly',\n",
       " 'to',\n",
       " 'its',\n",
       " 'success',\n",
       " 'ameager',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'continuation',\n",
       " 'of',\n",
       " 'my',\n",
       " 'career',\n",
       " 'in',\n",
       " 'dynamic',\n",
       " 'industry',\n",
       " 'maple',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'scientific',\n",
       " 'calculations',\n",
       " 'for',\n",
       " 'studying',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'value',\n",
       " 'property',\n",
       " 'of',\n",
       " 'working',\n",
       " 'across',\n",
       " 'distinct',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'pure',\n",
       " 'and',\n",
       " 'applied',\n",
       " 'mathematics',\n",
       " 'demonstrated',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'adopt',\n",
       " 'suitable',\n",
       " 'strategies',\n",
       " 'develop',\n",
       " 'new',\n",
       " 'techniques',\n",
       " 'and',\n",
       " 'think',\n",
       " '``',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'box',\n",
       " \"''\",\n",
       " 'use',\n",
       " 'logical',\n",
       " 'arguments',\n",
       " 'collaborating',\n",
       " 'with',\n",
       " 'engineers',\n",
       " 'on',\n",
       " 'the',\n",
       " 'resent',\n",
       " 'project',\n",
       " 'and',\n",
       " 'working',\n",
       " 'as',\n",
       " 'tutor',\n",
       " 'presented',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'and',\n",
       " 'give',\n",
       " 'feedback',\n",
       " 'on',\n",
       " 'the',\n",
       " 'work',\n",
       " 'tions',\n",
       " 'posters',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'written',\n",
       " 'editorial',\n",
       " 'and',\n",
       " 'public',\n",
       " 'speaking',\n",
       " 'skills',\n",
       " 'responding',\n",
       " 'appropriately',\n",
       " 'to',\n",
       " 'positive',\n",
       " 'or',\n",
       " 'negative',\n",
       " 'feedback',\n",
       " 'and',\n",
       " 'as',\n",
       " 'member',\n",
       " 'of',\n",
       " 'large',\n",
       " 'international',\n",
       " 'phd',\n",
       " 'programm',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'social',\n",
       " 'skills',\n",
       " 'active',\n",
       " 'listening',\n",
       " 'and',\n",
       " 'seeing',\n",
       " 'the',\n",
       " 'point',\n",
       " 'building',\n",
       " 'of',\n",
       " 'fruitful',\n",
       " 'collaboration',\n",
       " 'with',\n",
       " 'colleagues',\n",
       " 'london',\n",
       " 'teaching',\n",
       " 'associate',\n",
       " 'manchester',\n",
       " 'server',\n",
       " 'developer',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'microsoft',\n",
       " 'sql',\n",
       " 'server',\n",
       " 'and',\n",
       " 'postgresql',\n",
       " 'ticket',\n",
       " 'management',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'microsoft',\n",
       " '.net',\n",
       " 'web',\n",
       " 'based',\n",
       " 'asp',\n",
       " '.net',\n",
       " 'and',\n",
       " 'mvc',\n",
       " 'writing',\n",
       " 'and',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'apis',\n",
       " 'angular',\n",
       " 'jquery',\n",
       " 'javascript',\n",
       " 'css',\n",
       " 'microsoft',\n",
       " 'sql',\n",
       " 'server',\n",
       " 'and',\n",
       " 'postgresql',\n",
       " 'nunit',\n",
       " 'selenium',\n",
       " 'automated',\n",
       " 'testing',\n",
       " 'perforce',\n",
       " 'git',\n",
       " 'team',\n",
       " 'foundation',\n",
       " 'source',\n",
       " 'control',\n",
       " 'agile',\n",
       " 'scrum',\n",
       " 'development',\n",
       " 'methodology',\n",
       " 'ticket',\n",
       " 'management',\n",
       " 'go',\n",
       " 'live',\n",
       " 'support',\n",
       " 'during',\n",
       " 'phd',\n",
       " 'java',\n",
       " 'as',\n",
       " 'undergrad',\n",
       " 'assistant',\n",
       " 'manager',\n",
       " 'specialist',\n",
       " 'in',\n",
       " 'in-depth',\n",
       " 'inland',\n",
       " 'revenue',\n",
       " 'investigations',\n",
       " 'and',\n",
       " 'all',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'and',\n",
       " 'the',\n",
       " 'subsequent',\n",
       " 'completion',\n",
       " 'of',\n",
       " 'self-assessment',\n",
       " 'returns',\n",
       " 'general',\n",
       " 'windows',\n",
       " 'applications',\n",
       " 'psychologist',\n",
       " 'graduate',\n",
       " 'engineer',\n",
       " 'private',\n",
       " 'tutor',\n",
       " 'part',\n",
       " 'time',\n",
       " 'can',\n",
       " 'apply',\n",
       " 'mathematical',\n",
       " 'models',\n",
       " 'to',\n",
       " 'characterise',\n",
       " 'data',\n",
       " 'and',\n",
       " 'establish',\n",
       " 'trends',\n",
       " 'effective',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'communicator',\n",
       " 'articulate',\n",
       " 'with',\n",
       " 'extensive',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'scientific',\n",
       " 'writing',\n",
       " 'proof-reading',\n",
       " ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_data(**kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    \n",
    "#     data = pd.DataFrame(['asc','asda','asdasdasd'], columns=['value_char'])\n",
    "    \n",
    "    # singularize tokens\n",
    "#     data = data[prefix].map(lambda l: map(lambda w: w.singularize(), l))\n",
    "\n",
    "    # Spell correct flag\n",
    "    # REALLY SHOULD NEVER BE USED\n",
    "#     if spelling:\n",
    "#         print(\"Spell Correction Invoked.....\")\n",
    "#         data[prefix] = data[prefix].map(lambda l: map(lambda wl: map(lambda w: w.correct(), wl), l))\n",
    "#         print(data[prefix].map(lambda l: map(lambda w: type(w), l)))\n",
    "\n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "    aa = cleanestes.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    "\n",
    "    # remove tokens with length 1\n",
    "#     ree = re.compile(r'(\\'\\w)')\n",
    "#     rlen = len(data)\n",
    "#     tmp = data[prefix].copy()\n",
    "#     for index in range(0,rlen):\n",
    "#         wl_coll = list()\n",
    "#         for lst in tmp[index]:\n",
    "#             wl = list()\n",
    "#             for word in lst:\n",
    "#                 if not isinstance(word, types.NoneType):\n",
    "#                     if re.match(ree, word):\n",
    "#                         ree.sub('', word)\n",
    "#                     if len(word.strip().strip('.').strip(',')) > 1:\n",
    "#                         wl.append((word))\n",
    "#             wl_coll.append(WordList(wl))\n",
    "#         data[index] = wl_coll\n",
    "#     del tmp\n",
    "\n",
    "    # remove via regexp c'c pattern\n",
    "\n",
    "    # Stemming or lemmatization of tokens    \n",
    "    if normalizer == 'stem':\n",
    "        aa = aa.map(lambda l: map(lambda wl: map(lambda w: stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "#     elif normalizer == 'lemma':\n",
    "#         data[prefix] = data[prefix].map(lambda l: map(lambda wl: map(lambda w: w.lemmatize(), wl), l))\n",
    "#     elif normalizer == 'None':\n",
    "#         pass\n",
    "\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(Word, wl), l))\n",
    "#     data[prefix] = data[prefix].map(lambda l: map(WordList, l))\n",
    "    \n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_data(**tokenizer_prefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
