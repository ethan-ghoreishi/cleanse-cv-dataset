{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "import types\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.tokenize as nt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from textblob import blob, Blobber, TextBlob, Sentence, Word, WordList, tokenizers, sentiments, taggers, parsers, classifiers\n",
    "#from textblob_aptagger import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(dbname=\"skillsdb\",host=\"dw-instance.cbrlhmbtfrqg.eu-west-2.redshift.amazonaws.com\"\n",
    "                ,port=\"5439\",user=\"masteruser\", password=\"Ehgh1363\")\n",
    "curs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   5049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('''select count(distinct user_id) from cv''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_sql_query('''select distinct cv.user_id, cv_section_attribute.name,\n",
    "cv.value_char, cv.value_timestamp from cv_section_attribute \n",
    "left join cv on cv_section_attribute.id=cv.cv_section_attribute_id''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data['value_char'] = data['value_char'].map(lambda x: x.strip() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>value_char</th>\n",
       "      <th>value_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.257237e+11</td>\n",
       "      <td>name</td>\n",
       "      <td>Freelance Copy Editor of 100</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>grad_year</td>\n",
       "      <td>present</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        name                    value_char value_timestamp\n",
       "0  3.257237e+11        name  Freelance Copy Editor of 100             NaT\n",
       "1  1.987262e+11  admit_year                          None      2016-01-01\n",
       "2  1.987262e+11   grad_year                       present             NaT\n",
       "3  1.987262e+11  admit_year                          None      2015-01-01\n",
       "4  1.987262e+11  admit_year                          None      2012-01-01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>value_char</th>\n",
       "      <th>value_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.257237e+11</td>\n",
       "      <td>name</td>\n",
       "      <td>Freelance Copy Editor of 100</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>grad_year</td>\n",
       "      <td>present</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.987262e+11</td>\n",
       "      <td>admit_year</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        name                    value_char value_timestamp\n",
       "0  3.257237e+11        name  Freelance Copy Editor of 100             NaT\n",
       "1  1.987262e+11  admit_year                          None      2016-01-01\n",
       "2  1.987262e+11   grad_year                       present             NaT\n",
       "3  1.987262e+11  admit_year                          None      2015-01-01\n",
       "4  1.987262e+11  admit_year                          None      2012-01-01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid = data[data['name'].isin(['locale','name','summary','headline',\n",
    "                                     'degree','school','admit_year','grad_year',\n",
    "                                     'company',  'title',  'work_location',  \n",
    "                                     'start_date','end_date', 'description',\n",
    "                                     'award',\n",
    "                                     'publication', \n",
    "                                     'additional_info', \n",
    "                                     'skill'])]\n",
    "data_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(prefix=None):\n",
    "    '''\n",
    "    Cleans text data by:\n",
    "    1.  force lowercase\n",
    "    2.  remove non-ascii chars\n",
    "    3.  standardize whitespace\n",
    "    4.  remove digits\n",
    "    5.  remove control characters\n",
    "    6.  remove URL patterns\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        df = data_valid[prefix].dropna().map(lambda x: \"\".join(i for i in x.strip().lower() if ord(i)<128))\n",
    "    except UnicodeDecodeError:\n",
    "        print(UnicodeDecodeError)\n",
    "        df = data_valid[prefix].dropna().map(lambda x: x.strip().lower())\n",
    "\n",
    "        #     except Exception:\n",
    "#         print(Exception)\n",
    "#     finally:\n",
    "#         data[prefix]= data[prefix].map(lambda x: x.lower())\n",
    "\n",
    "    url_pattern = \"((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\"\n",
    "\n",
    "    re_URL = re.compile(url_pattern)\n",
    "#     re_TAG = re.compile(\"(<[phl]>)\", re.IGNORECASE)\n",
    "    re_WS = re.compile(\"/[^\\S\\n]/\")\n",
    "#     re_DIGIT = re.compile(\"\\d\")\n",
    "    re_CTRL = re.compile(\"[\\x00-\\x11\\x03-\\x1F]+\")\n",
    "    re_HI = re.compile(\"[\\x80-\\xFF]+\")\n",
    "    re_NWC = re.compile(\"[!;<>?{}\\/~`#=@#$%^&*()_+]\")\n",
    "    \n",
    "    df = df.map(lambda x: re_HI.sub(' ', x))\n",
    "    df = df.map(lambda x: re_CTRL.sub(' ', x))\n",
    "    df = df.map(lambda x: re_URL.sub(' ', x))\n",
    "#     data[prefix] = data[prefix].map(lambda x: re_DIGIT.sub(' ', x))\n",
    "    df = df.map(lambda x: re_WS.sub(' ', x))        \n",
    "    df = df.map(lambda x: re_NWC.sub(' ', x))\n",
    "    df = df.map(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             freelance copy editor of 100\n",
       "2                                  present\n",
       "5    research and commercialisation intern\n",
       "6             hold full uk driving license\n",
       "8            volunteer  campsite attendant\n",
       "Name: value_char, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('value_char').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>freelance copy editor of 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>research and commercialisation intern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hold full uk driving license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>volunteer  campsite attendant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              value_char\n",
       "0           freelance copy editor of 100\n",
       "2                                present\n",
       "5  research and commercialisation intern\n",
       "6           hold full uk driving license\n",
       "8          volunteer  campsite attendant"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame()\n",
    "data_clean['value_char'] = clean_text('value_char')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_valid = data_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Tokenization\n",
    "\n",
    "# # initialize the tokenizer\n",
    "\n",
    "# tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "# # tokenize data\n",
    "\n",
    "# data_clean['value_char'] = data_clean['value_char'].map(lambda x: tokenizer.tokenize(x.strip()))\n",
    "# data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word_tokenizer = nltk.tokenize.word_tokenize()\n",
    "# data_clean['value_char'] = data_clean['value_char'].map(lambda x: nltk.tokenize.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Stemming\n",
    "\n",
    "# # initialize the stemmer\n",
    "\n",
    "# stemmer = nltk.stem.PorterStemmer()\n",
    "# # data_clean['value_char'].map(lambda x: (i for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatize = nltk.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_headlines = data[data['name'] == 'headline'].reset_index()[['user_id','value_char']]\n",
    "# cv_degrees = data[data['name'] == 'degree'].reset_index()[['user_id','value_char']]\n",
    "# cv_schools = data[data['name'] == 'school'].reset_index()[['user_id','value_char']]\n",
    "# cv_locales = data[data['name'] == 'locale'].reset_index()[['user_id','value_char']]\n",
    "# cv_summaries = data[data['name'] == 'summary'].reset_index()[['user_id','value_char']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "tokenizer_prefs = {\n",
    "    'tokenizer' : nltk.tokenize.PunktSentenceTokenizer(),\n",
    "#     'token_format' : 'stem',\n",
    "    'spell_correct' : False,\n",
    "    'np_extract': None,\n",
    "    'pos_tagger': None,\n",
    "    'analyzer': None,\n",
    "    'classifier': None, \n",
    "    'clean_html': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_blob(prefix, **kwargs):\n",
    "    \n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    pos_tagger = kwargs['pos_tagger']\n",
    "    analyzer = kwargs['analyzer']\n",
    "    classifier = kwargs['classifier']\n",
    "    np_extract = kwargs['np_extract']\n",
    "    \n",
    "    blob = data_clean[prefix].map(lambda l: TextBlob(l,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                           np_extractor=np_extract,\n",
    "                                           pos_tagger=pos_tagger,\n",
    "                                           analyzer=analyzer,\n",
    "                                           classifier=classifier))\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (f, r, e, e, l, a, n, c, e,  , c, o, p, y,  , ...\n",
       "2                                 (p, r, e, s, e, n, t)\n",
       "5     (r, e, s, e, a, r, c, h,  , a, n, d,  , c, o, ...\n",
       "6     (h, o, l, d,  , f, u, l, l,  , u, k,  , d, r, ...\n",
       "8     (v, o, l, u, n, t, e, e, r,  ,  , c, a, m, p, ...\n",
       "9                                    (s, k, i, l, l, s)\n",
       "10    (l, i, g, h, t, -, s, c, a, t, t, e, r, i, n, ...\n",
       "13    (p, o, s, t, -, d, o, c, t, o, r, a, l,  , r, ...\n",
       "14    (t, e, a, m,  , l, e, a, d, e, r,  ,  ,  , n, ...\n",
       "15           (r, e, l, i, e, f,  , m, a, n, a, g, e, r)\n",
       "Name: value_char, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_blobs = pd.DataFrame()\n",
    "data_blobs['value_char'] = create_blob('value_char', **tokenizer_prefs)\n",
    "data_blobs.value_char[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(prefix):\n",
    "#     tokenizer = kwargs['tokenizer']\n",
    "#     normalizer = kwargs['token_format']\n",
    "\n",
    "    # tokenize the document into sentences from blob object\n",
    "    sentences = data_blobs[prefix].map(lambda s: s.sentences)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [(f, r, e, e, l, a, n, c, e,  , c, o, p, y,  ,...\n",
       "2                               [(p, r, e, s, e, n, t)]\n",
       "5     [(r, e, s, e, a, r, c, h,  , a, n, d,  , c, o,...\n",
       "6     [(h, o, l, d,  , f, u, l, l,  , u, k,  , d, r,...\n",
       "8     [(v, o, l, u, n, t, e, e, r,  ,  , c, a, m, p,...\n",
       "9                                  [(s, k, i, l, l, s)]\n",
       "10    [(l, i, g, h, t, -, s, c, a, t, t, e, r, i, n,...\n",
       "13    [(p, o, s, t, -, d, o, c, t, o, r, a, l,  , r,...\n",
       "14    [(t, e, a, m,  , l, e, a, d, e, r,  ,  ,  , n,...\n",
       "15         [(r, e, l, i, e, f,  , m, a, n, a, g, e, r)]\n",
       "Name: value_char, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentence_tokenized = pd.DataFrame()\n",
    "data_sentence_tokenized['value_char'] = tokenize_sentences('value_char')\n",
    "data_sentence_tokenized.value_char[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_words(prefix):\n",
    "#     tokenizer = kwargs['tokenizer']\n",
    "#     normalizer = kwargs['token_format']\n",
    "\n",
    "    # tokenize each sentence into words\n",
    "    # trim token whitespaces\n",
    "    # eliminate tokens of character length 1\n",
    "    #words = self.data[prefix].map(lambda w: w.strip().tokens if len(w)>1 else None)\n",
    "\n",
    "    words = data_sentence_tokenized[prefix].map(lambda l: map(lambda w: w.strip().words if len(w)>1 else None, l))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_word_tokenized = pd.DataFrame()\n",
    "data_word_tokenized = tokenize_words('value_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_data(data_word_tokenized):\n",
    "#     tokenizer = kwargs['tokenizer']\n",
    "#     normalizer = kwargs['token_format']\n",
    "#     spelling = kwargs['spell_correct']\n",
    "    \n",
    "    data_normalized = data_word_tokenized.map(lambda l: map(lambda w: w.singularize() if not isinstance(w, type(None)) and len(w)>1 else None, l))\n",
    "#     data_normalized = data_normalized.dropna()\n",
    "    \n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "#     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords and not isinstance(w, type(None)) else w, wl), l))\n",
    "#     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' and not isinstance(w, type(None) else w, wl), l))\n",
    "#     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' and not isinstance(w, type(None) else w, wl), l))\n",
    "#     data_normalized = data_normalized.dropna()\n",
    "\n",
    "     # remove tokens with length 1\n",
    "#     ree = re.compile(r'(\\'\\w)')\n",
    "#     rlen = len(data_normalized)\n",
    "#     tmp = data_normalized.copy()\n",
    "# #     tmp.reset_index(drop=True)\n",
    "#     for indx in tmp.index:\n",
    "#          wl_coll = list()\n",
    "#          for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "#             for j in i:\n",
    "#                 if not isinstance(j, type(None)):\n",
    "#                     for k in j:\n",
    "#                         if not isinstance(k, type(None)):\n",
    "#                             if re.match(ree, i):\n",
    "#                                 ree.sub('', i)\n",
    "#                             if len(i.strip().strip('.').strip(',')) > 1:\n",
    "#                                 wl.append((i))\n",
    "#                     wl_coll.append(WordList(wl))\n",
    "#             data_normalized[indx] = wl_col\n",
    "#             del tmp\n",
    "\n",
    "    # stemming\n",
    "#     data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: nltk.stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "\n",
    "#     data_word_tokenized= tokenize_words('value_char', **tokenizer_prefs)\n",
    "    \n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_normalized = normalize_data(data_word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if not isinstance(data_normalized, type(None)):\n",
    "#     for i in data_normalized:\n",
    "#         if not isinstance(i, type(None)):\n",
    "#             for j in i:\n",
    "#                 if not isinstance(j, type(None)):\n",
    "#                     for k in j:\n",
    "#                         if not isinstance(k, type(None)) and k not in stopWords and k != '\\'s' and k != '\\'d':\n",
    "#                             print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(data_normalized, target):\n",
    "    vocab = set()\n",
    "    if not isinstance(data_normalized, type(None)):\n",
    "        for token in data_normalized:\n",
    "            if not isinstance(token, type(None)):\n",
    "                for sent in token:\n",
    "                    if not isinstance(sent, type(None)):\n",
    "                        for word in sent:\n",
    "                            if not isinstance(word, type(None)) and word not in stopWords and word != '\\'s' and word != '\\'d':\n",
    "                                vocab.add(word)\n",
    "            \n",
    "    if target:\n",
    "        w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "        w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    else:\n",
    "        w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "    return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = build_vocab(data_normalized, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hypothesi': 2,\n",
       " 'ctrm': 11231,\n",
       " 'etching': 4,\n",
       " 'orthopedist': 5,\n",
       " 'premiere-based': 6,\n",
       " 'perday': 7,\n",
       " 'andposition': 8,\n",
       " 'dorado': 10,\n",
       " 'lorry': 11,\n",
       " 'gallup': 38168,\n",
       " 'semisolid': 12,\n",
       " '2013tinatus': 13,\n",
       " 'westlawelite': 14,\n",
       " 'sister': 15,\n",
       " 'became': 16,\n",
       " 'geometryal-qud': 17,\n",
       " 'vmsimplementing': 18,\n",
       " 'prosper': 19,\n",
       " 'sh': 20,\n",
       " 'rutgers-the': 21,\n",
       " 'dorotocephala': 22,\n",
       " 'windowsapplication': 23,\n",
       " '2015led': 24,\n",
       " '850,856': 28466,\n",
       " 'othersfocused': 25,\n",
       " 'sasproject': 9386,\n",
       " '429': 26,\n",
       " 'benefitsresource': 27,\n",
       " 'data-cabling': 28,\n",
       " 'worker-therapist': 29,\n",
       " 'thesis-analyzed': 28468,\n",
       " 'reportsquality': 31,\n",
       " 'steward': 33,\n",
       " 'teachingdecember': 34,\n",
       " '2016hillingdon': 38,\n",
       " 'r2application': 39,\n",
       " 'syspro': 45,\n",
       " 'deliving': 41,\n",
       " 't2': 42,\n",
       " 'compounddissolution': 44,\n",
       " 'whilstbeing': 9882,\n",
       " 'outlay': 9,\n",
       " '2764': 49151,\n",
       " 'emphasize': 46,\n",
       " \"'student\": 47,\n",
       " 'arts-political': 48,\n",
       " 'stagecommunicating': 52,\n",
       " 'severely': 51,\n",
       " 'radio-ligand': 18943,\n",
       " 'dtum': 53,\n",
       " 'clintrace': 9392,\n",
       " 'best-of': 54,\n",
       " 'instance': 55,\n",
       " 'financeuniversity': 9394,\n",
       " 'servicesprofessional': 56,\n",
       " 'accountingdocument': 57,\n",
       " 'lif': 51013,\n",
       " 'hello': 58,\n",
       " '250inmate': 59,\n",
       " 'continue': 60,\n",
       " 'militant': 55082,\n",
       " 'clr': 61,\n",
       " 'transferrable': 63,\n",
       " 'disassembly': 64,\n",
       " 'bike': 65,\n",
       " '2011financial': 66,\n",
       " 'wright': 37925,\n",
       " 'neurologic': 67,\n",
       " 'customizing': 68,\n",
       " 's26responsibility': 69,\n",
       " 'galloway': 37930,\n",
       " 'analysisof': 70,\n",
       " 'submitted': 71,\n",
       " 'drink': 9399,\n",
       " 'rfu': 72,\n",
       " 'kadry': 73,\n",
       " 'qtpsoftware': 74,\n",
       " 'caused': 75,\n",
       " 'freelancetrainer': 76,\n",
       " 'aktum': 77,\n",
       " 'statewide': 78,\n",
       " 'colorectal': 80,\n",
       " 'accounting-tbc': 47441,\n",
       " 'pro-on': 82,\n",
       " 'lomonosov': 9403,\n",
       " 'london-based': 83,\n",
       " 'conductedclient': 28478,\n",
       " 'mastcraft': 84,\n",
       " 'undermanned': 85,\n",
       " 'capitalanalysi': 88,\n",
       " 'git': 87,\n",
       " 'stroke': 91,\n",
       " 'mccordsville': 90,\n",
       " 'steel': 25162,\n",
       " 'crossvalidation': 51066,\n",
       " 'bramall': 92,\n",
       " 'bi-direction': 93,\n",
       " 'sputum': 94,\n",
       " 'meetcorporate': 4312,\n",
       " 'hauler': 95,\n",
       " 'strict': 96,\n",
       " '2014undergraduate': 97,\n",
       " 'microreactor': 98,\n",
       " 'elaborating': 99,\n",
       " 'e-signal': 100,\n",
       " 'storming': 9252,\n",
       " 'international': 101,\n",
       " 'achievablestandard': 102,\n",
       " 'richfield': 104,\n",
       " 'minuscule': 18953,\n",
       " '627': 107,\n",
       " 'ms-accesstesting': 106,\n",
       " '89-94': 108,\n",
       " 'oc4j': 109,\n",
       " 'mccarthy': 110,\n",
       " 'mcandrew': 111,\n",
       " 'geoved': 28481,\n",
       " 'bailey': 112,\n",
       " 'plug-in': 114,\n",
       " 'notable': 115,\n",
       " '15614': 117,\n",
       " 'shut': 28483,\n",
       " 'jhaco': 118,\n",
       " 'bluejuly': 119,\n",
       " 'multi-warehouseenvironment': 124,\n",
       " 'labconnect': 121,\n",
       " 'astute': 122,\n",
       " 'shipsformalized': 123,\n",
       " 'midatech': 47450,\n",
       " 'vision': 125,\n",
       " 'protocol': 52503,\n",
       " 'il-34': 9408,\n",
       " 'jnt': 128,\n",
       " 'nexusand': 127,\n",
       " 'yellowdot': 129,\n",
       " 'maximizer': 39020,\n",
       " 'flushed': 131,\n",
       " 'pressured': 133,\n",
       " 'disassembler': 134,\n",
       " 'defending': 135,\n",
       " 'agarwal': 136,\n",
       " 'superficial': 137,\n",
       " 'straus': 138,\n",
       " 'blocker': 139,\n",
       " 'consultantworked': 9414,\n",
       " 'hong': 140,\n",
       " 'accountscommunicated': 142,\n",
       " 'wicker': 143,\n",
       " 'sanef': 144,\n",
       " 'mef': 145,\n",
       " 'plc5': 16612,\n",
       " 'nut': 146,\n",
       " 'jit': 37944,\n",
       " 'shukla': 147,\n",
       " 'akademie': 152,\n",
       " 'seal': 149,\n",
       " 'mcpconfiguring': 150,\n",
       " 'kmhisi': 153,\n",
       " 'dataand': 154,\n",
       " 'spanish-english': 155,\n",
       " 'benefitmanagement': 156,\n",
       " 'plasmonic': 157,\n",
       " 'mention': 159,\n",
       " 'science-chemical': 160,\n",
       " 'backup-exec': 28486,\n",
       " '10-cm': 161,\n",
       " 'inflicted': 162,\n",
       " 'toilet': 163,\n",
       " 'hawkin': 164,\n",
       " 'bookersduty': 172,\n",
       " 'r2r': 170,\n",
       " 'learnerhighly': 168,\n",
       " 'babitum': 28490,\n",
       " 'townhouse': 173,\n",
       " 'byjanuary': 176,\n",
       " 'antineoplastic': 175,\n",
       " 'monetory': 177,\n",
       " 'itn': 178,\n",
       " 'tesco': 37948,\n",
       " 'silva': 179,\n",
       " 'lambert': 180,\n",
       " 'offlorida': 181,\n",
       " 'dated': 182,\n",
       " 'non-u': 47168,\n",
       " 'impedance': 183,\n",
       " 'mclaren': 185,\n",
       " 'pattinson': 51304,\n",
       " 'engl': 186,\n",
       " 'learn': 187,\n",
       " 'reductase': 189,\n",
       " 'bugzillaa': 18966,\n",
       " 'investing': 190,\n",
       " 'philadelphium': 191,\n",
       " 'micro-financing': 18969,\n",
       " 'innotec': 21351,\n",
       " 'sorrento': 47414,\n",
       " 'max': 47461,\n",
       " 'telcordium': 192,\n",
       " 'divinitati': 193,\n",
       " 'viewing': 194,\n",
       " '64th': 195,\n",
       " 'presentpreting': 18971,\n",
       " 'short-term': 196,\n",
       " 'cadd': 197,\n",
       " 'redirecting': 198,\n",
       " 'seite': 37954,\n",
       " 'tsafleb': 201,\n",
       " 'skillsexpert': 202,\n",
       " 'eagerly': 203,\n",
       " 'invocation': 28495,\n",
       " '23m': 204,\n",
       " 'bishop': 205,\n",
       " 'h2o': 206,\n",
       " 'externalongoing': 207,\n",
       " 'clean-desk': 208,\n",
       " 'average': 209,\n",
       " '55th': 210,\n",
       " 'welding': 211,\n",
       " 'strove': 212,\n",
       " '7798731067': 214,\n",
       " 'pentagon': 216,\n",
       " 'diverting': 218,\n",
       " 'kingdomnovember': 217,\n",
       " 'uphold': 219,\n",
       " 'wal-mart': 220,\n",
       " 'skava': 35,\n",
       " 'harrisburg': 222,\n",
       " 'vliegende': 223,\n",
       " 'experienceunix': 225,\n",
       " 'phosphospermine': 228,\n",
       " 'engineer-research': 229,\n",
       " 'solutionsdesigned': 32,\n",
       " 'trud': 230,\n",
       " 'for50': 231,\n",
       " 'reagent': 232,\n",
       " 'officedelivering': 235,\n",
       " 'cognate': 234,\n",
       " 'iiipace': 239,\n",
       " 'ministerial': 237,\n",
       " 'aif': 36,\n",
       " 'remington': 240,\n",
       " 'time-by': 242,\n",
       " 'fbusines': 243,\n",
       " 'multilingual': 244,\n",
       " 'hellerstein': 245,\n",
       " 'ending': 246,\n",
       " 'makongorosi': 247,\n",
       " 'phonetically': 248,\n",
       " 'analysisconsultantsagile': 37,\n",
       " 'exhibit': 31444,\n",
       " 'ln7075': 47473,\n",
       " 're-pricing': 251,\n",
       " 'roychowdhury': 250,\n",
       " 'shoob': 53741,\n",
       " 'alstom': 28504,\n",
       " 'multi-organ': 252,\n",
       " 'eventreporting': 253,\n",
       " 'eproject': 254,\n",
       " 'siebe': 255,\n",
       " 'clearing': 256,\n",
       " 'gida': 257,\n",
       " 'unidatum': 47476,\n",
       " 'freelancer': 258,\n",
       " 'rmysql': 259,\n",
       " 'ensheathing': 260,\n",
       " 'managementproces': 9426,\n",
       " 'judging': 261,\n",
       " 'university-metropolitan': 262,\n",
       " 'newbusines': 263,\n",
       " 'kuznetsov': 264,\n",
       " 'centeravaloq': 265,\n",
       " 'accountantconvenience': 266,\n",
       " 'sur': 268,\n",
       " 'kitnlp': 269,\n",
       " 'kudelski': 28510,\n",
       " 'exceeded': 270,\n",
       " 'sessional': 271,\n",
       " 'fp': 9428,\n",
       " 'mid1': 272,\n",
       " 'factor-1': 273,\n",
       " 'oncology': 274,\n",
       " 'rawalpindi': 47481,\n",
       " 'captioning': 40,\n",
       " 'analysislanguage': 275,\n",
       " 'gdansk': 276,\n",
       " 'gem': 277,\n",
       " '95willing': 28513,\n",
       " 'cpuc': 278,\n",
       " 'systempyxi': 279,\n",
       " 'englishadditional': 280,\n",
       " 'looming': 281,\n",
       " 'abi': 43,\n",
       " 'grigg': 282,\n",
       " 'strumica': 283,\n",
       " 'explorerdeltek': 284,\n",
       " 'bcg': 286,\n",
       " 'kalamazoo': 287,\n",
       " 'grayslake': 288,\n",
       " 're-negotiated': 289,\n",
       " 'equallogic': 290,\n",
       " 'memcache': 9432,\n",
       " 'gyro': 291,\n",
       " 'patio': 292,\n",
       " 'nue': 293,\n",
       " 'classifiero': 294,\n",
       " 'v1612': 295,\n",
       " 'either': 298,\n",
       " 'oif': 299,\n",
       " 'recrystallization': 300,\n",
       " 'cross-database': 305,\n",
       " 'grc': 303,\n",
       " 'mfr': 304,\n",
       " 'early-warning': 307,\n",
       " 'claims-c1-efs-sfdc': 53773,\n",
       " 'jrd': 54369,\n",
       " 'three-person': 308,\n",
       " 'manufacturejba': 45971,\n",
       " 'assembly': 309,\n",
       " 'endodonthic': 311,\n",
       " 'testopium': 312,\n",
       " 'pander': 313,\n",
       " '2006-august': 314,\n",
       " 'igrafix': 315,\n",
       " 'illionoi': 28518,\n",
       " 'thinkingjennifer': 316,\n",
       " 'policlinico': 317,\n",
       " 'usingvba': 318,\n",
       " 'testedcompound': 319,\n",
       " 'universally': 320,\n",
       " 're-education': 50966,\n",
       " 'luggage': 321,\n",
       " 'educationgrade': 49,\n",
       " 'compounds2004detection': 322,\n",
       " 'quality-assurance': 52832,\n",
       " 'distal': 19962,\n",
       " 'presentation-study': 44424,\n",
       " 'sapperson': 50,\n",
       " 'servermobile': 323,\n",
       " 'bioinformatician': 324,\n",
       " 'bdiploma': 325,\n",
       " 'branchburg': 37972,\n",
       " 'education-professional': 326,\n",
       " 'humoured': 13669,\n",
       " 'eev': 331,\n",
       " '120mmscf': 328,\n",
       " 'user-friend': 330,\n",
       " 'resolving': 31047,\n",
       " 'eye': 332,\n",
       " 'harmony': 333,\n",
       " 'alloy': 334,\n",
       " 'kingshurt': 28505,\n",
       " 'zro2': 335,\n",
       " 'forging': 337,\n",
       " 'topotential': 338,\n",
       " 'fmsfie': 342,\n",
       " 'polymeric': 340,\n",
       " 'brca1pedigree': 341,\n",
       " 'spomi': 343,\n",
       " 'analystdowner': 345,\n",
       " 'elicited': 346,\n",
       " 'spanish-speaking': 347,\n",
       " 'christleton': 348,\n",
       " 'suggestedthe': 9445,\n",
       " 'parenting': 18995,\n",
       " 'webproduct': 349,\n",
       " 'spectral': 350,\n",
       " 'viscoelastic': 351,\n",
       " 'factoranalysi': 18997,\n",
       " 'ubo': 9447,\n",
       " 'scrivner': 31216,\n",
       " 'changeover': 28529,\n",
       " '27th': 352,\n",
       " '06': 354,\n",
       " 'repression': 355,\n",
       " 'starobservation': 9448,\n",
       " 'councilconducted': 356,\n",
       " 'helico': 357,\n",
       " 'medimmune': 19002,\n",
       " 'phenomenon': 358,\n",
       " 'telepresence': 359,\n",
       " 'norlington': 19006,\n",
       " 'disapprove': 361,\n",
       " 'mnm': 362,\n",
       " 'nco': 9451,\n",
       " 'kick': 363,\n",
       " 'pharmacotherapy': 364,\n",
       " 'publicized': 365,\n",
       " 'techniqueso': 47502,\n",
       " 'rcf': 366,\n",
       " '5-10': 367,\n",
       " 'vbdatabasis': 368,\n",
       " 'lifepoint': 25497,\n",
       " 'ideal': 369,\n",
       " 'peritz': 54373,\n",
       " 'combing': 370,\n",
       " 'carrot-top': 371,\n",
       " 'insightmarvin-fresh': 373,\n",
       " 'sortino': 374,\n",
       " 'volunteer3rd': 28532,\n",
       " \"'test\": 375,\n",
       " 'un-necessary': 376,\n",
       " 'electricand': 377,\n",
       " 'uniliver': 378,\n",
       " 'db2environmentetl': 379,\n",
       " 'cif': 52212,\n",
       " 'designnational': 380,\n",
       " 'girl': 381,\n",
       " 'mgmtcomtrac': 382,\n",
       " 'systemsexcellent': 383,\n",
       " 'usingsqoop': 19009,\n",
       " 'perk': 62,\n",
       " 'work2v': 385,\n",
       " 'sqlmobile': 387,\n",
       " 'wella': 55926,\n",
       " 'non-traditional': 388,\n",
       " 'abilities12': 389,\n",
       " 'gwa': 390,\n",
       " 'genmod': 391,\n",
       " 'bayesprogramming': 392,\n",
       " 'oauth': 393,\n",
       " 'marnock': 395,\n",
       " 'trackman': 396,\n",
       " 'sdlchigh-level': 397,\n",
       " 'minipig': 398,\n",
       " 'enriched': 399,\n",
       " 'agf': 403,\n",
       " 'non-volatile': 402,\n",
       " 'smg': 37987,\n",
       " 'co-taught': 39268,\n",
       " 'lehigh': 405,\n",
       " 'plattsburgh': 407,\n",
       " 'dupont': 408,\n",
       " 'utilitydetail': 409,\n",
       " 'dominion': 413,\n",
       " 'comprehensivedental': 411,\n",
       " 'middleburg': 414,\n",
       " 'euthanasium': 28540,\n",
       " 'dispenser': 415,\n",
       " 'branding': 416,\n",
       " 'time-keeping': 417,\n",
       " 'manugistic': 418,\n",
       " 'outlook3d': 419,\n",
       " 'sparkcontext': 420,\n",
       " 'trench': 422,\n",
       " 'mitum': 51509,\n",
       " 'minimizing': 423,\n",
       " 'munich': 425,\n",
       " 'countydevelopmental': 426,\n",
       " 'onnationwide': 427,\n",
       " 'denial': 428,\n",
       " 'costing': 430,\n",
       " '2012provided': 433,\n",
       " 'tp': 435,\n",
       " 'perfected': 436,\n",
       " 'miop': 437,\n",
       " 'label-free': 439,\n",
       " 'hialeah': 440,\n",
       " 'orcongressional': 9464,\n",
       " 'etcstatistical': 13296,\n",
       " 'gastonium': 441,\n",
       " 'sort-order': 442,\n",
       " 'radrequirement': 443,\n",
       " 'adjust': 444,\n",
       " 'fast-frame': 47513,\n",
       " 'servicecommunication': 445,\n",
       " 'cpq': 37992,\n",
       " 'ch': 446,\n",
       " 'quoted': 447,\n",
       " 'webcast': 448,\n",
       " 'happen': 449,\n",
       " 'scientist-lab': 450,\n",
       " 'contentiou': 452,\n",
       " 'sieble': 453,\n",
       " 'itenology': 456,\n",
       " 'configuredhadoop': 19022,\n",
       " 'nslookup': 458,\n",
       " 'fed': 459,\n",
       " 'aeronautic': 460,\n",
       " 'il-12': 461,\n",
       " 'tsg101': 462,\n",
       " 'axion': 463,\n",
       " 'quarterjanuary': 465,\n",
       " 'issueswith': 466,\n",
       " 'non-degree': 468,\n",
       " 'realized': 469,\n",
       " 'credibility': 43957,\n",
       " 'pro-life': 47516,\n",
       " 'thinkingattention': 48170,\n",
       " 'administrationcore': 471,\n",
       " 'toughest': 472,\n",
       " 'physicalrehabilitation': 9465,\n",
       " 'millimeter': 473,\n",
       " 'uterine': 474,\n",
       " 'simmechanic': 475,\n",
       " 'palm': 476,\n",
       " 'balsamic': 477,\n",
       " 'offeror': 478,\n",
       " 'ideology': 479,\n",
       " 'oppositional': 7217,\n",
       " 'mopa': 480,\n",
       " 'durability': 481,\n",
       " 'pre-validation': 482,\n",
       " 'four-agency': 42279,\n",
       " 'rxkinetix': 483,\n",
       " 'sofium': 484,\n",
       " 'silvaco': 486,\n",
       " 'scotwork': 487,\n",
       " 'multiplex': 488,\n",
       " 'glenview': 489,\n",
       " 'yearsjava': 37999,\n",
       " 'standard': 490,\n",
       " 'gregory': 492,\n",
       " 'websitejeremy': 493,\n",
       " 'goodyear': 494,\n",
       " 'mada': 495,\n",
       " 'ealing': 496,\n",
       " 'hd-esr': 497,\n",
       " 'oracleinternethtml': 26210,\n",
       " 'securing': 498,\n",
       " 'solutionsand': 13136,\n",
       " 'gnu': 500,\n",
       " 'newpayment': 501,\n",
       " 'liquid-liquid': 28554,\n",
       " 'yulee': 503,\n",
       " 'moirastarted': 504,\n",
       " 'federal': 9473,\n",
       " 'targetted': 505,\n",
       " 'sign-offassisted': 506,\n",
       " 'billingsmonitored': 507,\n",
       " 'kadir': 508,\n",
       " 'moretransformational': 509,\n",
       " 'viavus': 512,\n",
       " 'castleton': 511,\n",
       " 'moreprofitable': 515,\n",
       " 'truncation': 513,\n",
       " 'surfaceregistration': 514,\n",
       " 'propelling': 517,\n",
       " '245kg': 518,\n",
       " '704': 519,\n",
       " 'gab1': 520,\n",
       " 'stressful': 521,\n",
       " 'ingenuity': 25377,\n",
       " 'offloadingnew': 522,\n",
       " 'word-processing': 19037,\n",
       " 'feedbackresult': 524,\n",
       " 'recourse': 525,\n",
       " 'presentpartner': 527,\n",
       " 'theopportunity': 528,\n",
       " 'gnp': 529,\n",
       " 'lavastorm': 530,\n",
       " '2,4': 19699,\n",
       " 'call': 531,\n",
       " 'andvideo': 532,\n",
       " 'sympathy': 533,\n",
       " 'taiga': 534,\n",
       " 'rt': 535,\n",
       " 'wing': 86,\n",
       " 'governed': 537,\n",
       " 'photocopier': 19042,\n",
       " '2013senior': 542,\n",
       " 'revenue-per-click': 541,\n",
       " 'icfai': 47529,\n",
       " 'helpidentify': 543,\n",
       " 'pre-cursor': 544,\n",
       " 'error-free': 545,\n",
       " 'ssc': 546,\n",
       " 'murray': 547,\n",
       " 'multi-award': 548,\n",
       " 'creator': 549,\n",
       " 'teampm': 19047,\n",
       " 'rabida': 554,\n",
       " 'located': 551,\n",
       " 'co-worker': 552,\n",
       " 'wy': 555,\n",
       " 'authenticity': 556,\n",
       " 'usedexpert': 557,\n",
       " 'successfully': 558,\n",
       " '0330': 9482,\n",
       " 'reviewing': 559,\n",
       " 'subacqueo': 560,\n",
       " 'ruffle': 19049,\n",
       " 'terrace': 561,\n",
       " 'naphthalene': 562,\n",
       " 'notedotherwise': 9488,\n",
       " 'user-training': 564,\n",
       " 'clickstream': 55941,\n",
       " 'peril': 567,\n",
       " 'larson': 566,\n",
       " 'lighthouse': 9486,\n",
       " 'onto': 568,\n",
       " 'molybdenum': 569,\n",
       " 'fundingdecision': 28561,\n",
       " 'bsi': 570,\n",
       " 'systemsn': 47533,\n",
       " 'franceproject': 571,\n",
       " '2001july': 572,\n",
       " 'vertica': 573,\n",
       " 'mappingsystem': 576,\n",
       " 'dreamspark': 579,\n",
       " 'corresponded': 586,\n",
       " 'gato': 585,\n",
       " 'webtrend': 583,\n",
       " 'token': 584,\n",
       " 'k-8': 587,\n",
       " 'pite': 590,\n",
       " 'companiesuk': 589,\n",
       " 'devicesperformed': 596,\n",
       " 'devry': 591,\n",
       " 'commercially': 592,\n",
       " 'protabit': 595,\n",
       " 'jh': 598,\n",
       " 'unixscript': 603,\n",
       " 'thrive': 599,\n",
       " 'adsorption': 600,\n",
       " 'majored': 602,\n",
       " 'kabul': 47540,\n",
       " 'c3': 605,\n",
       " 'editorial': 28563,\n",
       " 'theatre-level': 606,\n",
       " 'validity': 607,\n",
       " 'ppf': 608,\n",
       " 'pune': 610,\n",
       " 'aix-marseille': 614,\n",
       " 'partneringacquisitionsnegotiationstrade': 613,\n",
       " 'shaw': 615,\n",
       " 'depression': 616,\n",
       " 'afsimagevision': 617,\n",
       " 'dme': 618,\n",
       " 'internetadvertiser': 620,\n",
       " 'rule-based': 621,\n",
       " 'devise': 622,\n",
       " 'reformat': 623,\n",
       " 'opengli': 47543,\n",
       " 'coenzyme': 624,\n",
       " 'deodorant': 625,\n",
       " 'malaysium': 626,\n",
       " 're-order': 629,\n",
       " 'malibu': 630,\n",
       " 'applicationrole': 631,\n",
       " 'sourcepulse': 634,\n",
       " 'assayqualification': 635,\n",
       " 'usedleadershipcommunicationorganization': 636,\n",
       " '1mario': 9500,\n",
       " 'antiperspirancy': 637,\n",
       " 'linhardt': 638,\n",
       " 'picture-by-picture': 639,\n",
       " 'resale': 640,\n",
       " 'accessioned': 641,\n",
       " 'suggesting': 642,\n",
       " 'responsible': 103,\n",
       " 'usresponsibility': 647,\n",
       " 'utilise': 648,\n",
       " 'libor': 19060,\n",
       " 'issuednumerou': 650,\n",
       " '2017energypro': 651,\n",
       " '2006functional': 652,\n",
       " 'lanham': 653,\n",
       " 'depositor': 654,\n",
       " 'smoking': 655,\n",
       " 'managementfacilitation': 19062,\n",
       " 'mendocino': 38026,\n",
       " 'renovated': 656,\n",
       " 'eua': 658,\n",
       " 'royce': 659,\n",
       " 'xrd': 660,\n",
       " 'lung': 661,\n",
       " 'revolution': 662,\n",
       " 'companycarrying': 664,\n",
       " 'sportjune': 19066,\n",
       " 'topictaking': 665,\n",
       " 'maintenanceand': 666,\n",
       " 'sleek': 22980,\n",
       " 'controlaccredited': 9506,\n",
       " 'thesis': 667,\n",
       " '70,000': 668,\n",
       " 'sitefinity': 9507,\n",
       " 'snapshot': 669,\n",
       " 'post-test': 670,\n",
       " 'dana': 671,\n",
       " 'xtra': 19070,\n",
       " 'console': 8823,\n",
       " 'uplc-utilized': 28575,\n",
       " 'multipleself-employed': 47552,\n",
       " 'comprehensively': 672,\n",
       " 'substrate': 673,\n",
       " 'wolf': 674,\n",
       " 'loaf': 676,\n",
       " 'negligent': 677,\n",
       " 'list': 678,\n",
       " 'minneanalytic': 679,\n",
       " 'navfac': 681,\n",
       " 'screenshot': 682,\n",
       " 'warehousingrequirement': 683,\n",
       " 'chai': 684,\n",
       " 'modeller': 685,\n",
       " 'intermec': 686,\n",
       " '10khz': 687,\n",
       " 'significantfeature': 688,\n",
       " 'mongodbjira': 689,\n",
       " 'geo-referenced': 690,\n",
       " 'mciip': 691,\n",
       " '3214': 692,\n",
       " 'ccp3': 116,\n",
       " 'dischargebritish': 694,\n",
       " 'and1maintain': 28580,\n",
       " 'seniormanager': 695,\n",
       " 'cyberbullying': 696,\n",
       " 'viscou': 28579,\n",
       " 'brink': 697,\n",
       " 'datan': 698,\n",
       " 'toresource': 699,\n",
       " 'impacted': 700,\n",
       " 'mongodboperating': 36491,\n",
       " 'buchanan': 701,\n",
       " 'handlingorganisation': 703,\n",
       " 'ashra': 38036,\n",
       " '2024': 705,\n",
       " 'pre-test': 707,\n",
       " 'medoption': 708,\n",
       " 'expiring': 710,\n",
       " 'zyxel': 712,\n",
       " 'pre-selected': 713,\n",
       " 'passenglish': 715,\n",
       " 'nutrition': 716,\n",
       " 'nigel': 53581,\n",
       " 'rancher': 717,\n",
       " 'harriman': 47558,\n",
       " 'ews': 718,\n",
       " 'squarespace': 120,\n",
       " 'eyf': 719,\n",
       " 'numerou': 54922,\n",
       " '6-12': 720,\n",
       " 'exposurebioinformatic': 721,\n",
       " 'pearl': 722,\n",
       " 'ongoingprogres': 723,\n",
       " 'triatholon': 19080,\n",
       " 'calcareou': 725,\n",
       " 'vallabhbhai': 726,\n",
       " 'facilitatorexperienced': 728,\n",
       " 'missions2': 729,\n",
       " 'threatened': 731,\n",
       " 'costed': 732,\n",
       " 'infringement': 733,\n",
       " 'willask': 734,\n",
       " 'londonjune': 735,\n",
       " 'mosiero': 736,\n",
       " 'managementmanual': 737,\n",
       " 'feather': 28585,\n",
       " 'bfm': 739,\n",
       " 'batching': 742,\n",
       " 'lifespan': 741,\n",
       " 'ibioic': 743,\n",
       " 'requirement-by-requirement': 744,\n",
       " '2006epo': 745,\n",
       " 'triad': 746,\n",
       " 'alabama': 747,\n",
       " 'omniture': 748,\n",
       " 'glue': 48411,\n",
       " 'lawsstock': 749,\n",
       " 'waserstein': 55530,\n",
       " 'spanishproof-reading': 47566,\n",
       " 'predictionso': 750,\n",
       " 'servicesand': 751,\n",
       " 'linked-graph': 752,\n",
       " 'dose': 753,\n",
       " 'plese': 755,\n",
       " 'trivandrum': 756,\n",
       " 'durationipvpn': 757,\n",
       " '2000jira': 758,\n",
       " 'c260': 759,\n",
       " 'cyberlife': 47569,\n",
       " 'proof-of-concept': 760,\n",
       " 'guidechairing': 126,\n",
       " 'fiberglas': 761,\n",
       " 'confrontation': 130,\n",
       " 'detainee': 762,\n",
       " 'centerscheduler': 763,\n",
       " 'graphdatabasis': 764,\n",
       " 'preformed': 766,\n",
       " 'systemcascade': 767,\n",
       " 'chinainfrastructure': 769,\n",
       " 'cornerstone': 770,\n",
       " 'sqlplu': 771,\n",
       " 'photolithographyteaching': 773,\n",
       " 'upgrading': 774,\n",
       " 'bridgford': 775,\n",
       " 'universitt': 776,\n",
       " 'smi': 779,\n",
       " 'delicatessen': 780,\n",
       " 'day': 781,\n",
       " 'well-organised': 783,\n",
       " 'control-mrequirement': 784,\n",
       " 'dveloppement': 38050,\n",
       " 'nextech': 785,\n",
       " 'cadet': 47570,\n",
       " 'gimbal': 786,\n",
       " 'relying': 787,\n",
       " 'generatestimulu': 788,\n",
       " 'ftr': 41266,\n",
       " 'teambuilt': 789,\n",
       " 'phh': 790,\n",
       " 'simplicity': 791,\n",
       " 'analysisagile': 792,\n",
       " '2015xuber': 793,\n",
       " 'cartesi': 794,\n",
       " 'synthetases2017neurodegenerative': 796,\n",
       " 'contacting': 797,\n",
       " 'detaileddesign': 798,\n",
       " 'building': 799,\n",
       " 'sastek': 801,\n",
       " 'actionline': 802,\n",
       " 'enovium': 47572,\n",
       " 'determinant': 804,\n",
       " '208': 805,\n",
       " 'skillsproject': 807,\n",
       " \"company'spreliminary\": 808,\n",
       " 'non-material': 28600,\n",
       " '2biswajit': 810,\n",
       " '250m': 812,\n",
       " 'casino': 814,\n",
       " 'pastordeveloped': 813,\n",
       " 'dialect': 815,\n",
       " 'socializing': 816,\n",
       " 'mountbatten': 817,\n",
       " 'photographing': 818,\n",
       " 'packagei': 819,\n",
       " 'skadden': 820,\n",
       " 'educationjune': 822,\n",
       " 'clinicalfinding': 823,\n",
       " 'ruislip': 28601,\n",
       " 'minewa': 824,\n",
       " 'rheological': 826,\n",
       " 'institut': 827,\n",
       " 'trail': 828,\n",
       " 'comedic': 9538,\n",
       " 'increment': 829,\n",
       " 'puppet': 830,\n",
       " 'sartre': 831,\n",
       " 'shift': 38054,\n",
       " 'isproject': 832,\n",
       " 'evidentiaryobjection': 833,\n",
       " 'dealer': 38055,\n",
       " 'progression': 834,\n",
       " '1,771': 28604,\n",
       " 'skillsleadership': 835,\n",
       " 'platformproject': 45492,\n",
       " 'jagiellonian': 141,\n",
       " 'gromit': 838,\n",
       " 'ligament': 837,\n",
       " 'pgrouting': 9542,\n",
       " 'surf': 19098,\n",
       " '2013': 839,\n",
       " 'xpres': 840,\n",
       " 'k562': 841,\n",
       " 'cyber-physical': 38058,\n",
       " 'wascana': 842,\n",
       " 'nh': 843,\n",
       " 'ctgf': 844,\n",
       " 'competencies-effective': 845,\n",
       " 'suspicion': 846,\n",
       " 'insiebel': 847,\n",
       " 'v2v': 848,\n",
       " 'castellini': 850,\n",
       " 'staggering': 853,\n",
       " 'uscg': 851,\n",
       " 'highest-level': 852,\n",
       " 'asa': 854,\n",
       " 'ambrosium': 81,\n",
       " 'teamskill': 855,\n",
       " 'polysaccharide': 856,\n",
       " 'marketswith': 857,\n",
       " 'cardwell': 858,\n",
       " 'cardinal': 859,\n",
       " 'draftedmemoranda': 860,\n",
       " 'iberium': 861,\n",
       " '2016academic': 862,\n",
       " 'proforma': 863,\n",
       " 'brokerage': 864,\n",
       " 'java\\\\selenium': 865,\n",
       " 'designated': 866,\n",
       " 'sparx': 867,\n",
       " 'embrace': 868,\n",
       " 'amigo': 869,\n",
       " 'nstitution': 47583,\n",
       " 'equip': 870,\n",
       " 'short-sell': 148,\n",
       " 'argo': 872,\n",
       " 'implementationmy': 873,\n",
       " 'dcka': 47587,\n",
       " 'maintenancetemperature': 874,\n",
       " 'bio-similar': 875,\n",
       " 'bucharestduty': 151,\n",
       " 'willutilise': 876,\n",
       " 'andsource': 877,\n",
       " 'tabloid': 878,\n",
       " 'crew': 879,\n",
       " 'windchill': 880,\n",
       " 'peruse': 882,\n",
       " 'lithium': 884,\n",
       " 'insurancei': 886,\n",
       " 'whitley': 887,\n",
       " 'customerized': 888,\n",
       " 'andbehavior': 19104,\n",
       " 'kong': 889,\n",
       " 'kaarya': 4712,\n",
       " 'modeltesting': 24120,\n",
       " 'atdd': 891,\n",
       " 'sortprovider': 892,\n",
       " 'jennison': 893,\n",
       " 'powerpointsupervisor': 894,\n",
       " 'santander': 895,\n",
       " 'bpyc': 28618,\n",
       " 'nvidium': 19106,\n",
       " 'steering': 897,\n",
       " 'vt': 899,\n",
       " 'assignmentsmulti-taskingquick': 901,\n",
       " 'hydra': 902,\n",
       " 'biomedical': 903,\n",
       " 'lumension': 158,\n",
       " 'next-gen': 904,\n",
       " 'conform': 905,\n",
       " 'microteaching': 906,\n",
       " 'qlick': 3135,\n",
       " 'lanchester': 907,\n",
       " 'ctp1': 908,\n",
       " 'jee': 24002,\n",
       " 'provided': 909,\n",
       " 'yamunanagar': 913,\n",
       " 'eurocode': 911,\n",
       " 'supportother': 28621,\n",
       " 'coded': 914,\n",
       " 'jsonmicrosoft': 915,\n",
       " 'marszaek': 916,\n",
       " 'omni': 917,\n",
       " 'casesanswered': 28623,\n",
       " 'restrictive': 920,\n",
       " 'research-advanced': 921,\n",
       " 'tmp': 922,\n",
       " '2014-datum': 923,\n",
       " 'despair': 924,\n",
       " 'leavey': 925,\n",
       " 'biologyenzymatic': 926,\n",
       " 'scjp-certified': 927,\n",
       " 'bodi': 929,\n",
       " 'leadworked': 28627,\n",
       " 'vertex': 930,\n",
       " 'jba': 19112,\n",
       " 'meld': 931,\n",
       " 'nto': 933,\n",
       " 'begley': 935,\n",
       " 'extnd': 47601,\n",
       " 'ecg': 936,\n",
       " 'als70': 937,\n",
       " 'self-motivator': 938,\n",
       " 'adjustor': 941,\n",
       " 'prioitize': 940,\n",
       " 'cafepres': 48838,\n",
       " 'criminology': 943,\n",
       " 'alternativeway': 47602,\n",
       " 'ulf': 945,\n",
       " 'easyjet': 946,\n",
       " 'nanoscience': 947,\n",
       " 'unix-based': 948,\n",
       " 'soe': 950,\n",
       " 'andgoogle': 951,\n",
       " '2151': 56158,\n",
       " 'claimsproject': 952,\n",
       " 'semh': 3889,\n",
       " 'iparticipated': 953,\n",
       " 'tool-worklight': 954,\n",
       " 'evs1': 955,\n",
       " 'sanitation': 956,\n",
       " 'witham': 957,\n",
       " 'accomplishmentssurpas': 960,\n",
       " 'avenuenewcastle': 959,\n",
       " '3,400': 961,\n",
       " 'customer-orientatedcustomer': 962,\n",
       " 'cross-team': 963,\n",
       " '2016finance': 965,\n",
       " '10027': 966,\n",
       " 'miningprovide': 31021,\n",
       " 'classic': 967,\n",
       " 'exemplify': 968,\n",
       " 'vet': 20386,\n",
       " '2017i': 47610,\n",
       " 'moc': 969,\n",
       " 'lucidly': 970,\n",
       " 'danny': 900,\n",
       " 'light-weightdocumentationo': 971,\n",
       " 'vara': 972,\n",
       " 'imagework': 974,\n",
       " '2005-2006': 976,\n",
       " 'a3500austin': 165,\n",
       " 'signaller': 977,\n",
       " 'functionsn': 166,\n",
       " 'rome': 978,\n",
       " 'immediately': 167,\n",
       " 'outlook98': 979,\n",
       " 'bandit': 980,\n",
       " 'diploma-purchasing': 981,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-cda7dde5ecb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def build_vocab(data_word_tokenized, target):\n",
    "#     vocab = set()\n",
    "#     for token in data_word_tokenized:\n",
    "#         for sent in token:\n",
    "#             if not isinstance(sent, type(None)):\n",
    "#                 for word in sent:\n",
    "#                     vocab.add(word)\n",
    "            \n",
    "#     if target:\n",
    "#         w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "#         w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "#     else:\n",
    "#         w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "#     return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: map(lambda w: nltk.stem.PorterStemmer.NLTK_EXTENSIONS(w) if w in wl and not isinstance(w, type(None)) else wl.remove(w), wl), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-7f1cfeb94f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-82eefccd2ba1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_word_tokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in data_normalized:\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                for l in k:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeNoneTypes(lst):\n",
    "    return [i for i in lst if type(i) is not type(None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(data_word_tokenized, **kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    \n",
    "    data_normalized = data_word_tokenized.map(lambda l: map(lambda wl: removeNoneTypes(wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda w: w.singularize(), l))\n",
    "    \n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    " \n",
    "     # remove tokens with length 1\n",
    "#     ree = re.compile(r'(\\'\\w)')\n",
    "#     rlen = len(data_normalized)\n",
    "#     tmp = data_normalized.copy()\n",
    "# #     tmp.reset_index(drop=True)\n",
    "#     for indx in tmp.index:\n",
    "         wl_coll = list()\n",
    "         for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "            for j in i:\n",
    "                if not isinstance(j, type(None)):\n",
    "                    for k in j:\n",
    "                        if not isinstance(k, type(None)):\n",
    "                            if re.match(ree, i):\n",
    "                                ree.sub('', i)\n",
    "                            if len(i.strip().strip('.').strip(',')) > 1:\n",
    "                                wl.append((i))\n",
    "                    wl_coll.append(WordList(wl))\n",
    "            data_normalized[indx] = wl_col\n",
    "            del tmp\n",
    "\n",
    "    # stemming\n",
    "    data_normalized = data_normalized.map(lambda l: map(lambda wl: map(lambda w: nltk.stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "\n",
    "    data_word_tokenized= tokenize_words('value_char', **tokenizer_prefs)\n",
    "    \n",
    "    return data_word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "tokenizer  =   RegexpTokenizer(pattern=r'\\w+')\n",
    "stemmer    =   nltk.stem.PorterStemmer.NLTK_EXTENSIONS\n",
    "lemmatize  =   nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <map object at 0x7fbaa7e44550>\n",
       "1        <map object at 0x7fbaa7e445c0>\n",
       "2        <map object at 0x7fbaa7e44630>\n",
       "3        <map object at 0x7fbaa7e446d8>\n",
       "4        <map object at 0x7fbaa7e44780>\n",
       "6        <map object at 0x7fbaa7e44828>\n",
       "7        <map object at 0x7fbaa7e448d0>\n",
       "8        <map object at 0x7fbaa7e44978>\n",
       "9        <map object at 0x7fbaa7e44a20>\n",
       "10       <map object at 0x7fbaa7e44ac8>\n",
       "11       <map object at 0x7fbaa7e44b70>\n",
       "12       <map object at 0x7fbaa7e44c18>\n",
       "13       <map object at 0x7fbaa7e44cc0>\n",
       "14       <map object at 0x7fbaa7e44d68>\n",
       "18       <map object at 0x7fbaa7e44e10>\n",
       "19       <map object at 0x7fbaa7e44eb8>\n",
       "21       <map object at 0x7fbaa7e44f60>\n",
       "22       <map object at 0x7fbaa7e46048>\n",
       "23       <map object at 0x7fbaa7e460f0>\n",
       "24       <map object at 0x7fbaa7e46198>\n",
       "25       <map object at 0x7fbaa7e46240>\n",
       "29       <map object at 0x7fbaa7e462e8>\n",
       "30       <map object at 0x7fbaa7e46390>\n",
       "31       <map object at 0x7fbaa7e46438>\n",
       "32       <map object at 0x7fbaa7e464e0>\n",
       "33       <map object at 0x7fbaa7e46588>\n",
       "34       <map object at 0x7fbaa7e46630>\n",
       "35       <map object at 0x7fbaa7e466d8>\n",
       "36       <map object at 0x7fbaa7e46780>\n",
       "37       <map object at 0x7fbaa7e46828>\n",
       "                      ...              \n",
       "26744    <map object at 0x7fbaa778ceb8>\n",
       "26745    <map object at 0x7fbaa778cf60>\n",
       "26746    <map object at 0x7fbaa778e048>\n",
       "26747    <map object at 0x7fbaa778e0f0>\n",
       "26748    <map object at 0x7fbaa778e198>\n",
       "26749    <map object at 0x7fbaa778e240>\n",
       "26751    <map object at 0x7fbaa778e2e8>\n",
       "26752    <map object at 0x7fbaa778e390>\n",
       "26753    <map object at 0x7fbaa778e438>\n",
       "26754    <map object at 0x7fbaa778e4e0>\n",
       "26755    <map object at 0x7fbaa778e588>\n",
       "26756    <map object at 0x7fbaa778e630>\n",
       "26757    <map object at 0x7fbaa778e6d8>\n",
       "26758    <map object at 0x7fbaa778e780>\n",
       "26759    <map object at 0x7fbaa778e828>\n",
       "26760    <map object at 0x7fbaa778e8d0>\n",
       "26761    <map object at 0x7fbaa778e978>\n",
       "26762    <map object at 0x7fbaa778ea20>\n",
       "26763    <map object at 0x7fbaa778eac8>\n",
       "26764    <map object at 0x7fbaa778eb70>\n",
       "26765    <map object at 0x7fbaa778ec18>\n",
       "26767    <map object at 0x7fbaa778ecc0>\n",
       "26768    <map object at 0x7fbaa778ed68>\n",
       "26769    <map object at 0x7fbaa778ee10>\n",
       "26770    <map object at 0x7fbaa778eeb8>\n",
       "26771    <map object at 0x7fbaa778ef60>\n",
       "26772    <map object at 0x7fbaa7790048>\n",
       "26773    <map object at 0x7fbaa77900f0>\n",
       "26774    <map object at 0x7fbaa7790198>\n",
       "26775    <map object at 0x7fbaa7790240>\n",
       "Name: value_char, Length: 22845, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(data_word_tokenized,**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ree = re.compile(r'(\\'\\w)')\n",
    "wl = list()\n",
    "\n",
    "for i in normalize(data_word_tokenized,**tokenizer_prefs):\n",
    "    for j in i:\n",
    "        if not isinstance(j, type(None)):\n",
    "            for k in j:\n",
    "                if not isinstance(k, type(None)):\n",
    "                    if re.match(ree, k):\n",
    "                        ree.sub('', k)\n",
    "                    if len(k.strip().strip('.').strip(',')) > 1:\n",
    "                        wl.append((k))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mitcham',\n",
       " 'guest',\n",
       " 'lecturer',\n",
       " 'crewe',\n",
       " 'pencari',\n",
       " 'kerja',\n",
       " 'an',\n",
       " 'ambitious',\n",
       " 'and',\n",
       " 'hardworking',\n",
       " 'individual',\n",
       " 'who',\n",
       " 'is',\n",
       " 'motivated',\n",
       " 'by',\n",
       " 'challenge',\n",
       " 'and',\n",
       " 'is',\n",
       " 'passionate',\n",
       " 'to',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'inventor',\n",
       " 'on',\n",
       " 'four',\n",
       " 'patents',\n",
       " 'excellent',\n",
       " 'communicator',\n",
       " 'strong',\n",
       " 'planning',\n",
       " 'organisational',\n",
       " 'problem',\n",
       " 'solving',\n",
       " 'skills',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'successfully',\n",
       " 'analyse',\n",
       " 'and',\n",
       " 'assimilate',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'disparate',\n",
       " 'information',\n",
       " 'good',\n",
       " 'time',\n",
       " 'management',\n",
       " 'enjoys',\n",
       " 'working',\n",
       " 'under',\n",
       " 'pressure',\n",
       " 'and',\n",
       " 'to',\n",
       " 'deadlines',\n",
       " 'either',\n",
       " 'individually',\n",
       " 'or',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'team',\n",
       " 'conversational',\n",
       " 'german',\n",
       " 'part',\n",
       " 'time',\n",
       " 'lecturing',\n",
       " 'and',\n",
       " 'lab',\n",
       " 'demonstrator',\n",
       " 'project',\n",
       " 'evaluation',\n",
       " 'and',\n",
       " 'responsible',\n",
       " 'innovation',\n",
       " 'intern',\n",
       " 'computer',\n",
       " 'skills-',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'in',\n",
       " 'using',\n",
       " 'all',\n",
       " 'office',\n",
       " 'full',\n",
       " 'uk',\n",
       " 'driving',\n",
       " 'license',\n",
       " 'dewsbury',\n",
       " 'research',\n",
       " 'intern',\n",
       " 'micro',\n",
       " 'bio',\n",
       " 'nanofluidics',\n",
       " 'unit',\n",
       " 'attendant',\n",
       " 'lab',\n",
       " 'skills',\n",
       " 'spectroscopy',\n",
       " 'infra-red',\n",
       " 'spectroscopy',\n",
       " 'surface-tensiometry',\n",
       " 'conductivity',\n",
       " 'electron',\n",
       " 'microscopy',\n",
       " 'polymerisation',\n",
       " 'techniques',\n",
       " 'inc.',\n",
       " 'emulsion',\n",
       " 'dispersion',\n",
       " 'polymerisation',\n",
       " 'rheology',\n",
       " 'micro-piv',\n",
       " 'microfluidics',\n",
       " 'skillslight-scattering',\n",
       " 'techniques',\n",
       " 'inkjet',\n",
       " 'printing',\n",
       " 'nmr',\n",
       " 'size-exclusion',\n",
       " 'chromatograhy',\n",
       " 'uv-visible',\n",
       " 'spectroscopy',\n",
       " 'fluorescencelab',\n",
       " 'skills',\n",
       " 'spectroscopy',\n",
       " 'infra-red',\n",
       " 'spectroscopy',\n",
       " 'surface-tensiometry',\n",
       " 'conductivity',\n",
       " 'electron',\n",
       " 'microscopy',\n",
       " 'polymerisation',\n",
       " 'techniques',\n",
       " 'inc.',\n",
       " 'emulsion',\n",
       " 'dispersion',\n",
       " 'polymerisation',\n",
       " 'rheology',\n",
       " 'micro-piv',\n",
       " 'microfluidicscomputer',\n",
       " 'ms',\n",
       " 'office',\n",
       " 'ltex',\n",
       " 'inkscape',\n",
       " 'image',\n",
       " 'software',\n",
       " 'imagej',\n",
       " 'chembiooffice',\n",
       " 'origin',\n",
       " 'and',\n",
       " 'blender',\n",
       " 'basic',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'night',\n",
       " 'manager',\n",
       " 'london',\n",
       " 'excellent',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'skills',\n",
       " 'personnel',\n",
       " 'development',\n",
       " 'merchandising',\n",
       " 'staff',\n",
       " 'training',\n",
       " 'and',\n",
       " 'development',\n",
       " 'liverpool',\n",
       " 'postdoctoral',\n",
       " 'research',\n",
       " 'assistant',\n",
       " 'it',\n",
       " 'proficiency',\n",
       " 'and',\n",
       " 'programming',\n",
       " 'skills',\n",
       " 'matlab',\n",
       " 'creating',\n",
       " 'simulations',\n",
       " 'of',\n",
       " 'wave',\n",
       " 'propagation',\n",
       " 'on',\n",
       " 'flexural',\n",
       " 'systems',\n",
       " '2015',\n",
       " 'for',\n",
       " 'different',\n",
       " 'sequences',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'and',\n",
       " 'finite-state',\n",
       " 'automaton',\n",
       " 'simulators',\n",
       " 'numerical',\n",
       " 'solution',\n",
       " 'of',\n",
       " 'sql',\n",
       " 'started',\n",
       " 'to',\n",
       " 'study',\n",
       " 'coursera',\n",
       " '2016-',\n",
       " 'analytical',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'thinking',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'synthesize',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'data',\n",
       " 'range',\n",
       " 'of',\n",
       " 'activities',\n",
       " 'including',\n",
       " 'teaching',\n",
       " 'preparing',\n",
       " 'phd',\n",
       " 'thesis',\n",
       " 'research',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'presenta-',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'expertise',\n",
       " 'and',\n",
       " 'follow',\n",
       " 'leadership',\n",
       " 'directives',\n",
       " 'at',\n",
       " 'appropriate',\n",
       " 'times',\n",
       " 'marple',\n",
       " 'cheshire',\n",
       " 'uk',\n",
       " 'chemical',\n",
       " 'engineer',\n",
       " 'with',\n",
       " 'significant',\n",
       " 'experience',\n",
       " 'of',\n",
       " 'working',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'team',\n",
       " 'and',\n",
       " 'individually',\n",
       " 'in',\n",
       " 'timeline',\n",
       " 'driven',\n",
       " 'highly',\n",
       " 'pressurised',\n",
       " 'industrial',\n",
       " 'and',\n",
       " 'academic',\n",
       " 'environments',\n",
       " 'this',\n",
       " 'experience',\n",
       " 'has',\n",
       " 'been',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'large',\n",
       " 'range',\n",
       " 'of',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'science',\n",
       " 'and',\n",
       " 'biotechnology',\n",
       " 'following',\n",
       " 'several',\n",
       " 'bachelor',\n",
       " 'degrees',\n",
       " 'and',\n",
       " 'phd',\n",
       " 'in',\n",
       " 'chemical',\n",
       " 'engineering',\n",
       " 'this',\n",
       " 'has',\n",
       " 'manifested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'two',\n",
       " 'patent',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'which',\n",
       " 'am',\n",
       " 'co-inventor',\n",
       " 'this',\n",
       " 'industrial',\n",
       " 'experience',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'postgraduate',\n",
       " 'training',\n",
       " 'has',\n",
       " 'been',\n",
       " 'in',\n",
       " 'multi-disciplinary',\n",
       " 'environments',\n",
       " 'enabling',\n",
       " 'effective',\n",
       " 'communication',\n",
       " 'with',\n",
       " 'people',\n",
       " 'from',\n",
       " 'very',\n",
       " 'different',\n",
       " 'technical',\n",
       " 'and',\n",
       " 'non-technical',\n",
       " 'backgrounds',\n",
       " 'also',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'applying',\n",
       " 'for',\n",
       " 'chartership',\n",
       " 'london',\n",
       " 'registered',\n",
       " 'manager',\n",
       " 'london',\n",
       " 'demonstrator',\n",
       " 'chess',\n",
       " 'teacher',\n",
       " 'london',\n",
       " 'persian',\n",
       " 'native',\n",
       " 'general',\n",
       " 'operation',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'benfleet',\n",
       " 'matching',\n",
       " 'programming',\n",
       " 'matlab',\n",
       " 'scilab',\n",
       " 'shell',\n",
       " 'script',\n",
       " 'proficient',\n",
       " 'in',\n",
       " 'the',\n",
       " 'development',\n",
       " 'in',\n",
       " 'windows',\n",
       " 'and',\n",
       " 'linux',\n",
       " 'eigen',\n",
       " 'boost',\n",
       " 'opengl',\n",
       " 'etc',\n",
       " 'principal',\n",
       " 'scientist',\n",
       " 'sheffield',\n",
       " 'laboratory',\n",
       " 'demonstrator',\n",
       " 'legal',\n",
       " 'researcher',\n",
       " 'freelance',\n",
       " 'writer',\n",
       " 'all',\n",
       " 'areas',\n",
       " 'trainee',\n",
       " 'paralegal',\n",
       " 'have',\n",
       " 'excellent',\n",
       " 'skills',\n",
       " 'gained',\n",
       " 'through',\n",
       " 'regularly',\n",
       " 'using',\n",
       " 'window',\n",
       " 'based',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'microsoft',\n",
       " 'word',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'microsoft',\n",
       " 'powerpoint',\n",
       " 'computer',\n",
       " 'and',\n",
       " 'language',\n",
       " 'skills',\n",
       " 'have',\n",
       " 'excellent',\n",
       " 'skills',\n",
       " 'gained',\n",
       " 'through',\n",
       " 'regularly',\n",
       " 'using',\n",
       " 'window',\n",
       " 'based',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'microsoft',\n",
       " 'word',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'microsoft',\n",
       " 'powerpoint',\n",
       " 'have',\n",
       " 'obtained',\n",
       " 'certificate',\n",
       " 'in',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'at',\n",
       " 'level',\n",
       " 'can',\n",
       " 'read',\n",
       " 'speak',\n",
       " 'and',\n",
       " 'write',\n",
       " 'in',\n",
       " 'french',\n",
       " 'at',\n",
       " 'an',\n",
       " 'advanced',\n",
       " 'level',\n",
       " 'can',\n",
       " 'speak',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'tongue',\n",
       " 'language',\n",
       " 'punjabi',\n",
       " 'fluently',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'activities',\n",
       " 'am',\n",
       " 'and',\n",
       " 'enthusiastic',\n",
       " 'runner',\n",
       " 'am',\n",
       " 'greatly',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'mountain',\n",
       " 'climbing',\n",
       " 'as',\n",
       " 'recenlty',\n",
       " 'discovered',\n",
       " 'when',\n",
       " 'climbing',\n",
       " 'mount',\n",
       " 'sinai',\n",
       " 'in',\n",
       " 'north',\n",
       " 'africa',\n",
       " 'like',\n",
       " 'travelling',\n",
       " 'and',\n",
       " 'have',\n",
       " 'recently',\n",
       " 'been',\n",
       " 'backpacking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'artic',\n",
       " 'circle',\n",
       " 'languages',\n",
       " 'and',\n",
       " 'different',\n",
       " 'cultures',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'greatly',\n",
       " 'am',\n",
       " 'first',\n",
       " 'aider',\n",
       " 'hold',\n",
       " 'full',\n",
       " 'clean',\n",
       " 'driving',\n",
       " 'like',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'to',\n",
       " 'date',\n",
       " 'with',\n",
       " 'current',\n",
       " 'affairs',\n",
       " 'and',\n",
       " 'legal',\n",
       " 'issues',\n",
       " 'birmingham',\n",
       " 'consultant',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'developer',\n",
       " 'credit',\n",
       " 'risk',\n",
       " 'analyst',\n",
       " 'lavastorm',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'with',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'tools',\n",
       " 'such',\n",
       " 'as',\n",
       " 'qlikview',\n",
       " 'tableau',\n",
       " 'and',\n",
       " 'omniscope',\n",
       " 'cheltenham',\n",
       " 'adel',\n",
       " 'ia',\n",
       " 'marketing',\n",
       " 'manager',\n",
       " 'emea',\n",
       " 'norwich',\n",
       " 'associate',\n",
       " 'producer',\n",
       " 'production',\n",
       " 'manager',\n",
       " 'art',\n",
       " 'coordinator',\n",
       " 'associate',\n",
       " 'tutor',\n",
       " 'exam',\n",
       " 'invigilation',\n",
       " 'case',\n",
       " 'manager',\n",
       " 'job',\n",
       " 'seeker',\n",
       " 'member',\n",
       " 'of',\n",
       " 'school',\n",
       " 'executive',\n",
       " 'nottingham',\n",
       " 'talent',\n",
       " 'match',\n",
       " 'sheffield',\n",
       " 'city',\n",
       " 'region',\n",
       " 'programme',\n",
       " 'administrator',\n",
       " 'american',\n",
       " 'fork',\n",
       " 'ut',\n",
       " 'independent',\n",
       " 'consultant',\n",
       " 'global',\n",
       " 'health',\n",
       " 'sector',\n",
       " 'capacity',\n",
       " 'builder',\n",
       " 'lecturer',\n",
       " 'in',\n",
       " 'project',\n",
       " 'management',\n",
       " 'creative',\n",
       " 'and',\n",
       " 'well-rounded',\n",
       " 'graduate',\n",
       " 'with',\n",
       " 'first',\n",
       " 'class',\n",
       " 'honours',\n",
       " 'degree',\n",
       " 'in',\n",
       " 'english',\n",
       " 'and',\n",
       " 'creativewriting',\n",
       " 'and',\n",
       " 'masters',\n",
       " 'in',\n",
       " 'medical',\n",
       " 'humanities',\n",
       " 'currently',\n",
       " 'completing',\n",
       " 'phd',\n",
       " 'significant',\n",
       " 'experienceof',\n",
       " 'managing',\n",
       " 'and',\n",
       " 'motivating',\n",
       " 'large',\n",
       " 'teams',\n",
       " 'in',\n",
       " 'the',\n",
       " 'volunteering',\n",
       " 'and',\n",
       " 'charitable',\n",
       " 'sectors',\n",
       " 'exceptionalknowledge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'social',\n",
       " 'cultural',\n",
       " 'legislative',\n",
       " 'and',\n",
       " 'personal',\n",
       " 'issues',\n",
       " 'those',\n",
       " 'with',\n",
       " 'disabilities',\n",
       " 'face',\n",
       " 'attentive',\n",
       " 'todetail',\n",
       " 'with',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'freelance',\n",
       " 'editing',\n",
       " 'and',\n",
       " 'copywriting',\n",
       " 'confident',\n",
       " 'and',\n",
       " 'enthusiastic',\n",
       " 'with',\n",
       " 'excellentcommunication',\n",
       " 'skills',\n",
       " 'natural',\n",
       " 'leader',\n",
       " 'and',\n",
       " 'hands',\n",
       " 'on',\n",
       " 'team',\n",
       " 'worker',\n",
       " 'who',\n",
       " 'is',\n",
       " 'effective',\n",
       " 'at',\n",
       " 'time',\n",
       " 'managementand',\n",
       " 'presentations',\n",
       " 'to',\n",
       " 'groups',\n",
       " 'thrives',\n",
       " 'on',\n",
       " 'new',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'goals',\n",
       " 'part',\n",
       " 'time',\n",
       " 'lecturing',\n",
       " 'and',\n",
       " 'lab',\n",
       " 'demonstrator',\n",
       " 'leicestershire',\n",
       " 'uk',\n",
       " 'natural',\n",
       " 'products',\n",
       " 'research',\n",
       " 'assistant',\n",
       " 'computer',\n",
       " 'skills-',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'in',\n",
       " 'using',\n",
       " 'all',\n",
       " 'office',\n",
       " 'applications',\n",
       " 'volunteer',\n",
       " 'attendant',\n",
       " 'computer',\n",
       " 'ms',\n",
       " 'office',\n",
       " 'ltex',\n",
       " 'inkscape',\n",
       " 'image',\n",
       " 'software',\n",
       " 'imagej',\n",
       " 'chembiooffice',\n",
       " 'origin',\n",
       " 'and',\n",
       " 'blender',\n",
       " 'basic',\n",
       " 'bolton',\n",
       " 'supervisor',\n",
       " 'duty',\n",
       " 'manager',\n",
       " 'opening',\n",
       " 'and',\n",
       " 'closing',\n",
       " 'procedures',\n",
       " 'basic',\n",
       " 'food',\n",
       " 'hygiene',\n",
       " 'certificate',\n",
       " 'hospitality',\n",
       " 'course',\n",
       " 'national',\n",
       " 'license',\n",
       " 'certificate',\n",
       " 'phd',\n",
       " 'graduate',\n",
       " 'in',\n",
       " 'mathematics',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'position',\n",
       " 'in',\n",
       " 'global',\n",
       " 'company',\n",
       " 'that',\n",
       " 'works',\n",
       " 'on',\n",
       " 'real-worldproblems',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'use',\n",
       " 'my',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'expertise',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'significantly',\n",
       " 'to',\n",
       " 'its',\n",
       " 'success',\n",
       " 'ameager',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'continuation',\n",
       " 'of',\n",
       " 'my',\n",
       " 'career',\n",
       " 'in',\n",
       " 'dynamic',\n",
       " 'industry',\n",
       " 'maple',\n",
       " 'microsoft',\n",
       " 'excel',\n",
       " 'scientific',\n",
       " 'calculations',\n",
       " 'for',\n",
       " 'studying',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'value',\n",
       " 'property',\n",
       " 'of',\n",
       " 'working',\n",
       " 'across',\n",
       " 'distinct',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'pure',\n",
       " 'and',\n",
       " 'applied',\n",
       " 'mathematics',\n",
       " 'demonstrated',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'adopt',\n",
       " 'suitable',\n",
       " 'strategies',\n",
       " 'develop',\n",
       " 'new',\n",
       " 'techniques',\n",
       " 'and',\n",
       " 'think',\n",
       " '``',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'box',\n",
       " \"''\",\n",
       " 'use',\n",
       " 'logical',\n",
       " 'arguments',\n",
       " 'collaborating',\n",
       " 'with',\n",
       " 'engineers',\n",
       " 'on',\n",
       " 'the',\n",
       " 'resent',\n",
       " 'project',\n",
       " 'and',\n",
       " 'working',\n",
       " 'as',\n",
       " 'tutor',\n",
       " 'presented',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'and',\n",
       " 'give',\n",
       " 'feedback',\n",
       " 'on',\n",
       " 'the',\n",
       " 'work',\n",
       " 'tions',\n",
       " 'posters',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'written',\n",
       " 'editorial',\n",
       " 'and',\n",
       " 'public',\n",
       " 'speaking',\n",
       " 'skills',\n",
       " 'responding',\n",
       " 'appropriately',\n",
       " 'to',\n",
       " 'positive',\n",
       " 'or',\n",
       " 'negative',\n",
       " 'feedback',\n",
       " 'and',\n",
       " 'as',\n",
       " 'member',\n",
       " 'of',\n",
       " 'large',\n",
       " 'international',\n",
       " 'phd',\n",
       " 'programm',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'my',\n",
       " 'social',\n",
       " 'skills',\n",
       " 'active',\n",
       " 'listening',\n",
       " 'and',\n",
       " 'seeing',\n",
       " 'the',\n",
       " 'point',\n",
       " 'building',\n",
       " 'of',\n",
       " 'fruitful',\n",
       " 'collaboration',\n",
       " 'with',\n",
       " 'colleagues',\n",
       " 'london',\n",
       " 'teaching',\n",
       " 'associate',\n",
       " 'manchester',\n",
       " 'server',\n",
       " 'developer',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'microsoft',\n",
       " 'sql',\n",
       " 'server',\n",
       " 'and',\n",
       " 'postgresql',\n",
       " 'ticket',\n",
       " 'management',\n",
       " 'key',\n",
       " 'skills',\n",
       " 'microsoft',\n",
       " '.net',\n",
       " 'web',\n",
       " 'based',\n",
       " 'asp',\n",
       " '.net',\n",
       " 'and',\n",
       " 'mvc',\n",
       " 'writing',\n",
       " 'and',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'apis',\n",
       " 'angular',\n",
       " 'jquery',\n",
       " 'javascript',\n",
       " 'css',\n",
       " 'microsoft',\n",
       " 'sql',\n",
       " 'server',\n",
       " 'and',\n",
       " 'postgresql',\n",
       " 'nunit',\n",
       " 'selenium',\n",
       " 'automated',\n",
       " 'testing',\n",
       " 'perforce',\n",
       " 'git',\n",
       " 'team',\n",
       " 'foundation',\n",
       " 'source',\n",
       " 'control',\n",
       " 'agile',\n",
       " 'scrum',\n",
       " 'development',\n",
       " 'methodology',\n",
       " 'ticket',\n",
       " 'management',\n",
       " 'go',\n",
       " 'live',\n",
       " 'support',\n",
       " 'during',\n",
       " 'phd',\n",
       " 'java',\n",
       " 'as',\n",
       " 'undergrad',\n",
       " 'assistant',\n",
       " 'manager',\n",
       " 'specialist',\n",
       " 'in',\n",
       " 'in-depth',\n",
       " 'inland',\n",
       " 'revenue',\n",
       " 'investigations',\n",
       " 'and',\n",
       " 'all',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'and',\n",
       " 'the',\n",
       " 'subsequent',\n",
       " 'completion',\n",
       " 'of',\n",
       " 'self-assessment',\n",
       " 'returns',\n",
       " 'general',\n",
       " 'windows',\n",
       " 'applications',\n",
       " 'psychologist',\n",
       " 'graduate',\n",
       " 'engineer',\n",
       " 'private',\n",
       " 'tutor',\n",
       " 'part',\n",
       " 'time',\n",
       " 'can',\n",
       " 'apply',\n",
       " 'mathematical',\n",
       " 'models',\n",
       " 'to',\n",
       " 'characterise',\n",
       " 'data',\n",
       " 'and',\n",
       " 'establish',\n",
       " 'trends',\n",
       " 'effective',\n",
       " 'and',\n",
       " 'confident',\n",
       " 'communicator',\n",
       " 'articulate',\n",
       " 'with',\n",
       " 'extensive',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'scientific',\n",
       " 'writing',\n",
       " 'proof-reading',\n",
       " ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_data(**kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    normalizer = kwargs['token_format']\n",
    "    spelling = kwargs['spell_correct']\n",
    "    \n",
    "#     data = pd.DataFrame(['asc','asda','asdasdasd'], columns=['value_char'])\n",
    "    \n",
    "    # singularize tokens\n",
    "#     data = data[prefix].map(lambda l: map(lambda w: w.singularize(), l))\n",
    "\n",
    "    # Spell correct flag\n",
    "    # REALLY SHOULD NEVER BE USED\n",
    "#     if spelling:\n",
    "#         print(\"Spell Correction Invoked.....\")\n",
    "#         data[prefix] = data[prefix].map(lambda l: map(lambda wl: map(lambda w: w.correct(), wl), l))\n",
    "#         print(data[prefix].map(lambda l: map(lambda w: type(w), l)))\n",
    "\n",
    "    # filter out 'bad' words, normalize good ones\n",
    "    # w if w not in self.stopWords else wl.remove(w)\n",
    "    aa = cleanestes.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w in stopWords else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'s' else w, wl), l))\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(lambda w: wl.remove(w) if w == '\\'d' else w, wl), l))\n",
    "\n",
    "    # remove tokens with length 1\n",
    "#     ree = re.compile(r'(\\'\\w)')\n",
    "#     rlen = len(data)\n",
    "#     tmp = data[prefix].copy()\n",
    "#     for index in range(0,rlen):\n",
    "#         wl_coll = list()\n",
    "#         for lst in tmp[index]:\n",
    "#             wl = list()\n",
    "#             for word in lst:\n",
    "#                 if not isinstance(word, types.NoneType):\n",
    "#                     if re.match(ree, word):\n",
    "#                         ree.sub('', word)\n",
    "#                     if len(word.strip().strip('.').strip(',')) > 1:\n",
    "#                         wl.append((word))\n",
    "#             wl_coll.append(WordList(wl))\n",
    "#         data[index] = wl_coll\n",
    "#     del tmp\n",
    "\n",
    "    # remove via regexp c'c pattern\n",
    "\n",
    "    # Stemming or lemmatization of tokens    \n",
    "    if normalizer == 'stem':\n",
    "        aa = aa.map(lambda l: map(lambda wl: map(lambda w: stemmer.stem(w) if w in wl and not isinstance(w, types.NoneType) else wl.remove(w), wl), l))\n",
    "#     elif normalizer == 'lemma':\n",
    "#         data[prefix] = data[prefix].map(lambda l: map(lambda wl: map(lambda w: w.lemmatize(), wl), l))\n",
    "#     elif normalizer == 'None':\n",
    "#         pass\n",
    "\n",
    "    aa = aa.map(lambda l: map(lambda wl: map(Word, wl), l))\n",
    "#     data[prefix] = data[prefix].map(lambda l: map(WordList, l))\n",
    "    \n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_data(**tokenizer_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
